# Module 05: ë”¥ëŸ¬ë‹ ê¸°ë°˜ ìŠ¤í…Œë ˆì˜¤ (RAFT-Stereo, AANet)

[![ë‚œì´ë„](https://img.shields.io/badge/ë‚œì´ë„-â­â­â­â­_ê³ ê¸‰-red.svg)]()
[![ì˜ˆìƒì‹œê°„](https://img.shields.io/badge/ì˜ˆìƒì‹œê°„-15--20ì‹œê°„-blue.svg)]()
[![ì„ ìˆ˜ì§€ì‹](https://img.shields.io/badge/ì„ ìˆ˜ì§€ì‹-Module_03,_PyTorch_ê¸°ì´ˆ-orange.svg)]()

---

## ğŸ“‹ ëª¨ë“ˆ ê°œìš”

| í•­ëª© | ë‚´ìš© |
|------|------|
| **í•™ìŠµ ëª©í‘œ** | ìµœì‹  ë”¥ëŸ¬ë‹ ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ ëª¨ë¸ ì´í•´ ë° í™œìš© |
| **í•µì‹¬ í‚¤ì›Œë“œ** | RAFT-Stereo, AANet, Cost Volume, Disparity Regression |
| **ì‚°ì¶œë¬¼** | ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê¹Šì´ ì¶”ì • ì‹œìŠ¤í…œ, ì„±ëŠ¥ ë¹„êµ ë¶„ì„ |

---

## ğŸ“š ëª©ì°¨

1. [ë”¥ëŸ¬ë‹ ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ ê°œìš”](#1-ë”¥ëŸ¬ë‹-ìŠ¤í…Œë ˆì˜¤-ë§¤ì¹­-ê°œìš”)
2. [í•µì‹¬ ê°œë…](#2-í•µì‹¬-ê°œë…)
3. [í™˜ê²½ ì„¤ì •](#3-í™˜ê²½-ì„¤ì •)
4. [RAFT-Stereo](#4-raft-stereo)
5. [AANet](#5-aanet)
6. [ê¸°íƒ€ ì£¼ìš” ëª¨ë¸](#6-ê¸°íƒ€-ì£¼ìš”-ëª¨ë¸)
7. [ì „í†µì  ë°©ë²• vs ë”¥ëŸ¬ë‹ ë¹„êµ](#7-ì „í†µì -ë°©ë²•-vs-ë”¥ëŸ¬ë‹-ë¹„êµ)
8. [ì»¤ìŠ¤í…€ ë°ì´í„° ì ìš©](#8-ì»¤ìŠ¤í…€-ë°ì´í„°-ì ìš©)
9. [ì„±ëŠ¥ ìµœì í™”](#9-ì„±ëŠ¥-ìµœì í™”)
10. [ì‹¤ìŠµ í”„ë¡œì íŠ¸](#10-ì‹¤ìŠµ-í”„ë¡œì íŠ¸)

---

## 1. ë”¥ëŸ¬ë‹ ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ ê°œìš”

### 1.1 ì „í†µì  ë°©ë²•ì˜ í•œê³„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ì „í†µì  ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ì˜ í•œê³„                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  âŒ í…ìŠ¤ì²˜ ì—†ëŠ” ì˜ì—­ (ë²½, í•˜ëŠ˜)                              â”‚
â”‚     â†’ ë§¤ì¹­ ì‹¤íŒ¨, êµ¬ë© ë°œìƒ                                   â”‚
â”‚                                                             â”‚
â”‚  âŒ ë°˜ë³µ íŒ¨í„´ (íƒ€ì¼, ìš¸íƒ€ë¦¬)                                 â”‚
â”‚     â†’ ì˜ëª»ëœ ë§¤ì¹­, ë…¸ì´ì¦ˆ                                    â”‚
â”‚                                                             â”‚
â”‚  âŒ ë°˜ì‚¬/íˆ¬ëª… í‘œë©´ (ìœ ë¦¬, ë¬¼)                                â”‚
â”‚     â†’ ë¶ˆì•ˆì •í•œ ê²°ê³¼                                         â”‚
â”‚                                                             â”‚
â”‚  âŒ ê°€ë ¤ì§„ ì˜ì—­ (Occlusion)                                 â”‚
â”‚     â†’ ì •ë³´ ë¶€ì¡±                                             â”‚
â”‚                                                             â”‚
â”‚  âŒ ìˆ˜ë™ íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”                                   â”‚
â”‚     â†’ í™˜ê²½ë§ˆë‹¤ ì¬ì¡°ì •                                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ë”¥ëŸ¬ë‹ ì ‘ê·¼ë²•ì˜ ì¥ì 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ë”¥ëŸ¬ë‹ ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ì˜ ì¥ì                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  âœ… í•™ìŠµëœ íŠ¹ì§• ì¶”ì¶œ                                         â”‚
â”‚     â†’ í…ìŠ¤ì²˜ ì—†ëŠ” ì˜ì—­ì—ì„œë„ ë¬¸ë§¥ íŒŒì•…                        â”‚
â”‚                                                             â”‚
â”‚  âœ… End-to-End í•™ìŠµ                                         â”‚
â”‚     â†’ ìë™ ìµœì í™”, íŒŒë¼ë¯¸í„° íŠœë‹ ë¶ˆí•„ìš”                       â”‚
â”‚                                                             â”‚
â”‚  âœ… ëŒ€ê·œëª¨ ë°ì´í„° í•™ìŠµ                                       â”‚
â”‚     â†’ ë‹¤ì–‘í•œ ìƒí™©ì— ì¼ë°˜í™”                                   â”‚
â”‚                                                             â”‚
â”‚  âœ… ì„œë¸Œí”½ì…€ ì •í™•ë„                                          â”‚
â”‚     â†’ íšŒê·€ ê¸°ë°˜ìœ¼ë¡œ ì—°ì†ì ì¸ ì‹œì°¨ ì¶œë ¥                        â”‚
â”‚                                                             â”‚
â”‚  âœ… ê°€ë ¤ì§„ ì˜ì—­ ì²˜ë¦¬                                         â”‚
â”‚     â†’ ë¬¸ë§¥ ì •ë³´ë¡œ ì¶”ë¡                                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ì£¼ìš” ë”¥ëŸ¬ë‹ ëª¨ë¸ íƒ€ì„ë¼ì¸

```
2016    2017    2018    2019    2020    2021    2022    2023
  â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
  â–¼       â–¼       â–¼       â–¼       â–¼       â–¼       â–¼       â–¼
DispNet  GC-Net  PSMNet  GA-Net  AANet  RAFT-   IGEV   ì„ í–‰
         â”‚               â”‚       â”‚     Stereo  Stereo  ì—°êµ¬
         â”‚               â”‚       â”‚       â”‚       â”‚
    3D Conv         Attention  ì ì‘í˜•  ë°˜ë³µì    ê¸°í•˜í•™
    Cost Volume     ë©”ì»¤ë‹ˆì¦˜   ì§‘ê³„   ì—…ë°ì´íŠ¸  ë³¼ë¥¨
```

### 1.4 ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (KITTI 2015 ë²¤ì¹˜ë§ˆí¬)

| ëª¨ë¸ | D1-all (%) | ì¶”ë¡  ì‹œê°„ | íŠ¹ì§• |
|------|-----------|----------|------|
| SGM (ì „í†µ) | 10.86 | ~1s | ë² ì´ìŠ¤ë¼ì¸ |
| PSMNet | 2.32 | ~0.4s | í”¼ë¼ë¯¸ë“œ í’€ë§ |
| GA-Net | 1.81 | ~1.8s | ê°€ì´ë“œ ì§‘ê³„ |
| **AANet** | 2.03 | **0.07s** | ë¹ ë¥¸ ì†ë„ |
| **RAFT-Stereo** | **1.27** | ~0.3s | ìµœê³  ì •í™•ë„ |
| IGEV-Stereo | 1.12 | ~0.2s | ìµœì‹  SOTA |

---

## 2. í•µì‹¬ ê°œë…

### 2.1 Cost Volume

ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ì˜ í•µì‹¬ ë°ì´í„° êµ¬ì¡°:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cost Volume êµ¬ì¡°                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   ì™¼ìª½ íŠ¹ì§• ë§µ         ì˜¤ë¥¸ìª½ íŠ¹ì§• ë§µ                         â”‚
â”‚   [H Ã— W Ã— C]          [H Ã— W Ã— C]                         â”‚
â”‚       â”‚                    â”‚                               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                â–¼                                            â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚        â”‚  Cost Volume  â”‚                                   â”‚
â”‚        â”‚ [D Ã— H Ã— W Ã— C]â”‚  D: ì‹œì°¨ ë²”ìœ„                     â”‚
â”‚        â”‚               â”‚  H: ë†’ì´                          â”‚
â”‚        â”‚   d=0 â”€â”€â”€â”€â”€â”€â”€â”€â”‚  W: ë„ˆë¹„                          â”‚
â”‚        â”‚   d=1 â”€â”€â”€â”€â”€â”€â”€â”€â”‚  C: ì±„ë„ (íŠ¹ì§• ì°¨ì´/ìƒê´€)           â”‚
â”‚        â”‚   ...         â”‚                                   â”‚
â”‚        â”‚   d=D â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                   â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                â”‚                                            â”‚
â”‚                â–¼                                            â”‚
â”‚        3D CNN / GRUë¡œ ì²˜ë¦¬                                  â”‚
â”‚                â”‚                                            â”‚
â”‚                â–¼                                            â”‚
â”‚        Disparity Map [H Ã— W]                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Cost Volume êµ¬ì„± ë°©ë²•

```python
"""
cost_volume_example.py
Cost Volume êµ¬ì„± ì˜ˆì‹œ
"""

import torch
import torch.nn.functional as F


def build_concat_volume(left_feat, right_feat, max_disp):
    """
    Concatenation ê¸°ë°˜ Cost Volume
    
    Parameters:
    - left_feat: [B, C, H, W] ì™¼ìª½ íŠ¹ì§• ë§µ
    - right_feat: [B, C, H, W] ì˜¤ë¥¸ìª½ íŠ¹ì§• ë§µ
    - max_disp: ìµœëŒ€ ì‹œì°¨
    
    Returns:
    - cost_volume: [B, 2C, D, H, W]
    """
    
    B, C, H, W = left_feat.shape
    cost_volume = torch.zeros(B, 2*C, max_disp, H, W, device=left_feat.device)
    
    for d in range(max_disp):
        if d == 0:
            cost_volume[:, :C, d, :, :] = left_feat
            cost_volume[:, C:, d, :, :] = right_feat
        else:
            cost_volume[:, :C, d, :, d:] = left_feat[:, :, :, d:]
            cost_volume[:, C:, d, :, d:] = right_feat[:, :, :, :-d]
    
    return cost_volume


def build_correlation_volume(left_feat, right_feat, max_disp):
    """
    Correlation ê¸°ë°˜ Cost Volume
    
    Returns:
    - cost_volume: [B, D, H, W]
    """
    
    B, C, H, W = left_feat.shape
    cost_volume = torch.zeros(B, max_disp, H, W, device=left_feat.device)
    
    for d in range(max_disp):
        if d == 0:
            cost_volume[:, d, :, :] = (left_feat * right_feat).sum(dim=1)
        else:
            cost_volume[:, d, :, d:] = (left_feat[:, :, :, d:] * right_feat[:, :, :, :-d]).sum(dim=1)
    
    # ì •ê·œí™”
    cost_volume = cost_volume / C
    
    return cost_volume


def build_group_correlation_volume(left_feat, right_feat, max_disp, num_groups=8):
    """
    Group-wise Correlation (GwcNet ìŠ¤íƒ€ì¼)
    
    ì±„ë„ì„ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìƒê´€ ê³„ì‚°
    â†’ ë” í’ë¶€í•œ ë§¤ì¹­ ì •ë³´
    
    Returns:
    - cost_volume: [B, G, D, H, W]
    """
    
    B, C, H, W = left_feat.shape
    assert C % num_groups == 0
    
    channels_per_group = C // num_groups
    cost_volume = torch.zeros(B, num_groups, max_disp, H, W, device=left_feat.device)
    
    # ê·¸ë£¹ìœ¼ë¡œ ë¶„í• 
    left_groups = left_feat.view(B, num_groups, channels_per_group, H, W)
    right_groups = right_feat.view(B, num_groups, channels_per_group, H, W)
    
    for d in range(max_disp):
        if d == 0:
            cost_volume[:, :, d, :, :] = (left_groups * right_groups).sum(dim=2)
        else:
            cost_volume[:, :, d, :, d:] = (left_groups[:, :, :, :, d:] * 
                                           right_groups[:, :, :, :, :-d]).sum(dim=2)
    
    cost_volume = cost_volume / channels_per_group
    
    return cost_volume
```

### 2.3 Disparity Regression

```python
"""
disparity_regression.py
ì‹œì°¨ íšŒê·€ ë°©ë²•
"""

import torch
import torch.nn.functional as F


def soft_argmin(cost_volume, temperature=1.0):
    """
    Soft Argmin (Differentiable Argmin)
    
    Costë¥¼ í™•ë¥ ë¡œ ë³€í™˜ í›„ ê¸°ëŒ“ê°’ ê³„ì‚°
    â†’ ì„œë¸Œí”½ì…€ ì •í™•ë„ì˜ ì‹œì°¨ ì¶œë ¥
    
    Parameters:
    - cost_volume: [B, D, H, W] ë¹„ìš© ë³¼ë¥¨
    - temperature: softmax ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ sharp)
    
    Returns:
    - disparity: [B, H, W] ì‹œì°¨ ë§µ
    """
    
    B, D, H, W = cost_volume.shape
    
    # Softmaxë¡œ í™•ë¥  ë³€í™˜ (ë¹„ìš©ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ í™•ë¥ )
    prob = F.softmax(-cost_volume / temperature, dim=1)  # [B, D, H, W]
    
    # ì‹œì°¨ ì¸ë±ìŠ¤
    disp_candidates = torch.arange(D, device=cost_volume.device, dtype=torch.float32)
    disp_candidates = disp_candidates.view(1, D, 1, 1).expand(B, D, H, W)
    
    # ê¸°ëŒ“ê°’ ê³„ì‚°
    disparity = (prob * disp_candidates).sum(dim=1)  # [B, H, W]
    
    return disparity


def disparity_regression_uncertainty(cost_volume):
    """
    ë¶ˆí™•ì‹¤ì„±ê³¼ í•¨ê»˜ ì‹œì°¨ íšŒê·€
    
    Returns:
    - disparity: ì‹œì°¨ ë§µ
    - uncertainty: ë¶ˆí™•ì‹¤ì„± ë§µ (ë¶„ì‚°)
    """
    
    B, D, H, W = cost_volume.shape
    
    prob = F.softmax(-cost_volume, dim=1)
    
    disp_candidates = torch.arange(D, device=cost_volume.device, dtype=torch.float32)
    disp_candidates = disp_candidates.view(1, D, 1, 1).expand(B, D, H, W)
    
    # í‰ê·  (ì‹œì°¨)
    disparity = (prob * disp_candidates).sum(dim=1)
    
    # ë¶„ì‚° (ë¶ˆí™•ì‹¤ì„±)
    disparity_expanded = disparity.unsqueeze(1).expand(B, D, H, W)
    variance = (prob * (disp_candidates - disparity_expanded) ** 2).sum(dim=1)
    uncertainty = torch.sqrt(variance)
    
    return disparity, uncertainty
```

---

## 3. í™˜ê²½ ì„¤ì •

### 3.1 ìš”êµ¬ì‚¬í•­

```bash
# í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- GPU: NVIDIA GPU (ìµœì†Œ 6GB VRAM, ê¶Œì¥ 8GB+)
- CUDA: 11.0 ì´ìƒ

# ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
Python >= 3.8
PyTorch >= 1.10
torchvision >= 0.11
CUDA Toolkit >= 11.0
```

### 3.2 í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
conda create -n stereo_dl python=3.9
conda activate stereo_dl

# PyTorch ì„¤ì¹˜ (CUDA 11.8 ê¸°ì¤€)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install numpy opencv-python matplotlib tqdm pyyaml

# Open3D (í¬ì¸íŠ¸ í´ë¼ìš°ë“œìš©)
pip install open3d

# RAFT-Stereo ì¶”ê°€ ì˜ì¡´ì„±
pip install scipy tensorboard

# AANet ì¶”ê°€ ì˜ì¡´ì„±
pip install scikit-image
```

### 3.3 GPU í™•ì¸

```python
"""
check_gpu.py
GPU í™˜ê²½ í™•ì¸
"""

import torch

def check_gpu():
    print("="*50)
    print("GPU í™˜ê²½ í™•ì¸")
    print("="*50)
    
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"\nGPU {i}: {props.name}")
            print(f"  ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.1f} GB")
            print(f"  Compute Capability: {props.major}.{props.minor}")
    else:
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦¼).")

if __name__ == "__main__":
    check_gpu()
```

---

## 4. RAFT-Stereo

### 4.1 ëª¨ë¸ ê°œìš”

RAFT-StereoëŠ” RAFT (Recurrent All-Pairs Field Transforms)ë¥¼ ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ì— ì ìš©í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAFT-Stereo ì•„í‚¤í…ì²˜                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   ì™¼ìª½ ì´ë¯¸ì§€ â”€â”€â†’ Feature     â”€â”€â”                           â”‚
â”‚                 Encoder         â”‚                           â”‚
â”‚                                 â”œâ”€â”€â†’ Correlation Volume     â”‚
â”‚   ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ â”€â”€â†’ Feature    â”€â”€â”˜    (All-Pairs)           â”‚
â”‚                 Encoder                   â”‚                 â”‚
â”‚                                           â”‚                 â”‚
â”‚   ì™¼ìª½ ì´ë¯¸ì§€ â”€â”€â†’ Context     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                 â”‚
â”‚                 Encoder                 â”‚ â”‚                 â”‚
â”‚                                         â–¼ â–¼                 â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                              â”‚   GRU Update     â”‚ â†â”€â”       â”‚
â”‚                              â”‚   (ë°˜ë³µì  ì •ì œ)   â”‚   â”‚       â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚       â”‚
â”‚                                       â”‚             â”‚       â”‚
â”‚                                       â–¼             â”‚       â”‚
â”‚                              Disparity Update â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                       â”‚                     â”‚
â”‚                                       â–¼ (Në²ˆ ë°˜ë³µ)          â”‚
â”‚                              Final Disparity Map            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 í•µì‹¬ êµ¬ì„±ìš”ì†Œ

1. **Feature Encoder**: ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
2. **Context Encoder**: ë¬¸ë§¥ ì •ë³´ ì¶”ì¶œ
3. **Correlation Volume**: ëª¨ë“  í”½ì…€ ìŒì˜ ìƒê´€ë„ ê³„ì‚°
4. **GRU Update**: ë°˜ë³µì ìœ¼ë¡œ ì‹œì°¨ ì¶”ì • ì •ì œ

### 4.3 RAFT-Stereo ì‚¬ìš©ë²•

```python
"""
raft_stereo_inference.py
RAFT-Stereo ì¶”ë¡  ì½”ë“œ
"""

import sys
import torch
import numpy as np
import cv2
from pathlib import Path

# RAFT-Stereo ê²½ë¡œ ì¶”ê°€ (í´ë¡  í›„)
# git clone https://github.com/princeton-vl/RAFT-Stereo.git
sys.path.append('RAFT-Stereo')

from core.raft_stereo import RAFTStereo
from core.utils.utils import InputPadder


class RAFTStereoInference:
    def __init__(self, model_path, device='cuda'):
        """
        RAFT-Stereo ì¶”ë¡  í´ë˜ìŠ¤
        
        Parameters:
        - model_path: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ
        - device: 'cuda' ë˜ëŠ” 'cpu'
        """
        
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’)
        class Args:
            corr_implementation = "reg"  # ë˜ëŠ” "alt" (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            shared_backbone = False
            corr_levels = 4
            corr_radius = 4
            n_downsample = 2
            context_norm = "batch"
            slow_fast_gru = False
            n_gru_layers = 3
            hidden_dims = [128] * 3
            mixed_precision = False
        
        self.args = Args()
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = RAFTStereo(self.args)
        
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint, strict=False)
        
        self.model = self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… RAFT-Stereo ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        print(f"   Device: {self.device}")
    
    def preprocess(self, img_left, img_right):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        
        # BGR â†’ RGB
        if len(img_left.shape) == 3 and img_left.shape[2] == 3:
            img_left = cv2.cvtColor(img_left, cv2.COLOR_BGR2RGB)
            img_right = cv2.cvtColor(img_right, cv2.COLOR_BGR2RGB)
        
        # numpy â†’ tensor
        img_left = torch.from_numpy(img_left).permute(2, 0, 1).float()
        img_right = torch.from_numpy(img_right).permute(2, 0, 1).float()
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        img_left = img_left.unsqueeze(0).to(self.device)
        img_right = img_right.unsqueeze(0).to(self.device)
        
        # íŒ¨ë”© (8ì˜ ë°°ìˆ˜ë¡œ)
        padder = InputPadder(img_left.shape, divis_by=32)
        img_left, img_right = padder.pad(img_left, img_right)
        
        return img_left, img_right, padder
    
    @torch.no_grad()
    def inference(self, img_left, img_right, iters=32):
        """
        ì‹œì°¨ ë§µ ì¶”ë¡ 
        
        Parameters:
        - img_left: ì™¼ìª½ ì´ë¯¸ì§€ (BGR, numpy)
        - img_right: ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ (BGR, numpy)
        - iters: GRU ë°˜ë³µ íšŸìˆ˜ (ë§ì„ìˆ˜ë¡ ì •í™•, ëŠë¦¼)
        
        Returns:
        - disparity: ì‹œì°¨ ë§µ (numpy, float32)
        """
        
        # ì „ì²˜ë¦¬
        left_tensor, right_tensor, padder = self.preprocess(img_left, img_right)
        
        # ì¶”ë¡ 
        _, flow_up = self.model(left_tensor, right_tensor, iters=iters, test_mode=True)
        
        # í›„ì²˜ë¦¬
        disparity = -flow_up.squeeze().cpu().numpy()
        
        # íŒ¨ë”© ì œê±°
        h, w = img_left.shape[:2]
        disparity = disparity[:h, :w]
        
        return disparity
    
    def inference_with_uncertainty(self, img_left, img_right, iters=32, num_samples=5):
        """
        ë¶ˆí™•ì‹¤ì„± ì¶”ì •ê³¼ í•¨ê»˜ ì¶”ë¡  (MC Dropout ìŠ¤íƒ€ì¼)
        
        Parameters:
        - num_samples: ìƒ˜í”Œ ìˆ˜
        
        Returns:
        - disparity_mean: í‰ê·  ì‹œì°¨ ë§µ
        - disparity_std: í‘œì¤€í¸ì°¨ (ë¶ˆí™•ì‹¤ì„±)
        """
        
        disparities = []
        
        for _ in range(num_samples):
            disp = self.inference(img_left, img_right, iters=iters)
            disparities.append(disp)
        
        disparities = np.stack(disparities, axis=0)
        disparity_mean = np.mean(disparities, axis=0)
        disparity_std = np.std(disparities, axis=0)
        
        return disparity_mean, disparity_std


def demo_raft_stereo():
    """RAFT-Stereo ë°ëª¨"""
    
    print("="*60)
    print("RAFT-Stereo ë°ëª¨")
    print("="*60)
    
    # ëª¨ë¸ ê²½ë¡œ (ë‹¤ìš´ë¡œë“œ í•„ìš”)
    # https://github.com/princeton-vl/RAFT-Stereo/releases
    model_path = "models/raftstereo-realtime.pth"
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
    img_left = cv2.imread("test_left.png")
    img_right = cv2.imread("test_right.png")
    
    if img_left is None or img_right is None:
        print("í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¶”ë¡ 
    raft = RAFTStereoInference(model_path)
    disparity = raft.inference(img_left, img_right, iters=32)
    
    print(f"\nì‹œì°¨ ë²”ìœ„: {disparity.min():.1f} ~ {disparity.max():.1f}")
    
    # ì‹œê°í™”
    disp_vis = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX)
    disp_color = cv2.applyColorMap(disp_vis.astype(np.uint8), cv2.COLORMAP_MAGMA)
    
    cv2.imwrite("raft_stereo_result.png", disp_color)
    print("âœ… ì €ì¥ë¨: raft_stereo_result.png")


if __name__ == "__main__":
    demo_raft_stereo()
```

### 4.4 ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# RAFT-Stereo ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/princeton-vl/RAFT-Stereo.git
cd RAFT-Stereo

# ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
mkdir -p models
cd models

# ETH3D í›ˆë ¨ ëª¨ë¸ (ê³ ì •ë°€)
wget https://www.dropbox.com/s/xxx/raftstereo-eth3d.pth

# Middlebury í›ˆë ¨ ëª¨ë¸
wget https://www.dropbox.com/s/xxx/raftstereo-middlebury.pth

# ì‹¤ì‹œê°„ ë²„ì „ (ë¹ ë¦„, ì •í™•ë„ ì•½ê°„ ë‚®ìŒ)
wget https://www.dropbox.com/s/xxx/raftstereo-realtime.pth
```

---

## 5. AANet

### 5.1 ëª¨ë¸ ê°œìš”

AANet (Adaptive Aggregation Network)ì€ ë¹ ë¥¸ ì†ë„ì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸ì…ë‹ˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AANet ì•„í‚¤í…ì²˜                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   ì™¼ìª½/ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€                                          â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚ Feature     â”‚  ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ                     â”‚
â”‚   â”‚ Extraction  â”‚                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚ Cost Volume â”‚  ìƒê´€ ê¸°ë°˜ (3D Conv ì—†ìŒ!)                 â”‚
â”‚   â”‚ (Sparse)    â”‚  â†’ ë©”ëª¨ë¦¬ íš¨ìœ¨ì                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚ Adaptive    â”‚  ISA: Intra-Scale Aggregation            â”‚
â”‚   â”‚ Aggregation â”‚  CSA: Cross-Scale Aggregation            â”‚
â”‚   â”‚ Module      â”‚  â†’ ë³€í˜• ê°€ëŠ¥í•œ í•©ì„±ê³± ì‚¬ìš©                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚ Disparity   â”‚  Soft Argmin                             â”‚
â”‚   â”‚ Regression  â”‚                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚   Final Disparity Map                                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 AANetì˜ í•µì‹¬ íŠ¹ì§•

1. **No 3D Convolution**: ë©”ëª¨ë¦¬ì™€ ì†ë„ íš¨ìœ¨
2. **Adaptive Aggregation**: ë³€í˜• ê°€ëŠ¥í•œ í•©ì„±ê³±ìœ¼ë¡œ ìœ ì—°í•œ ì§‘ê³„
3. **Multi-scale**: ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ì •ë³´ í™œìš©

### 5.3 AANet ì‚¬ìš©ë²•

```python
"""
aanet_inference.py
AANet ì¶”ë¡  ì½”ë“œ
"""

import sys
import torch
import numpy as np
import cv2

# AANet ê²½ë¡œ ì¶”ê°€
# git clone https://github.com/haofeixu/aanet.git
sys.path.append('aanet')

from nets import AANet


class AANetInference:
    def __init__(self, model_path, max_disp=192, device='cuda'):
        """
        AANet ì¶”ë¡  í´ë˜ìŠ¤
        
        Parameters:
        - model_path: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ
        - max_disp: ìµœëŒ€ ì‹œì°¨
        - device: 'cuda' ë˜ëŠ” 'cpu'
        """
        
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.max_disp = max_disp
        
        # ëª¨ë¸ ìƒì„±
        self.model = AANet(
            max_disp=max_disp,
            num_downsample=2,
            feature_type='aanet',
            no_feature_mdconv=False,
            feature_pyramid=False,
            feature_pyramid_network=True,
            feature_similarity='correlation',
            aggregation_type='adaptive',
            num_scales=3,
            num_fusions=6,
            num_stage_blocks=1,
            num_deform_blocks=3,
            no_intermediate_supervision=True,
            refinement_type='stereodrnet',
            mdconv_dilation=2,
            deformable_groups=2
        )
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['state_dict'], strict=False)
        
        self.model = self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… AANet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        print(f"   ìµœëŒ€ ì‹œì°¨: {max_disp}")
        print(f"   Device: {self.device}")
    
    def preprocess(self, img_left, img_right):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        
        # BGR â†’ RGB
        img_left = cv2.cvtColor(img_left, cv2.COLOR_BGR2RGB)
        img_right = cv2.cvtColor(img_right, cv2.COLOR_BGR2RGB)
        
        # ì •ê·œí™”
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        
        img_left = img_left.astype(np.float32) / 255.0
        img_right = img_right.astype(np.float32) / 255.0
        
        img_left = (img_left - mean) / std
        img_right = (img_right - mean) / std
        
        # numpy â†’ tensor
        img_left = torch.from_numpy(img_left).permute(2, 0, 1).float()
        img_right = torch.from_numpy(img_right).permute(2, 0, 1).float()
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        img_left = img_left.unsqueeze(0).to(self.device)
        img_right = img_right.unsqueeze(0).to(self.device)
        
        return img_left, img_right
    
    def pad_to_multiple(self, img, multiple=64):
        """ì´ë¯¸ì§€ë¥¼ multipleì˜ ë°°ìˆ˜ë¡œ íŒ¨ë”©"""
        
        _, _, h, w = img.shape
        
        new_h = ((h - 1) // multiple + 1) * multiple
        new_w = ((w - 1) // multiple + 1) * multiple
        
        pad_h = new_h - h
        pad_w = new_w - w
        
        img_padded = torch.nn.functional.pad(img, (0, pad_w, 0, pad_h))
        
        return img_padded, (h, w)
    
    @torch.no_grad()
    def inference(self, img_left, img_right):
        """
        ì‹œì°¨ ë§µ ì¶”ë¡ 
        
        Parameters:
        - img_left: ì™¼ìª½ ì´ë¯¸ì§€ (BGR, numpy)
        - img_right: ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ (BGR, numpy)
        
        Returns:
        - disparity: ì‹œì°¨ ë§µ (numpy, float32)
        """
        
        original_h, original_w = img_left.shape[:2]
        
        # ì „ì²˜ë¦¬
        left_tensor, right_tensor = self.preprocess(img_left, img_right)
        
        # íŒ¨ë”©
        left_tensor, _ = self.pad_to_multiple(left_tensor, 64)
        right_tensor, _ = self.pad_to_multiple(right_tensor, 64)
        
        # ì¶”ë¡ 
        pred_disp = self.model(left_tensor, right_tensor)[-1]  # ë§ˆì§€ë§‰ ìŠ¤ì¼€ì¼
        
        # í›„ì²˜ë¦¬
        disparity = pred_disp.squeeze().cpu().numpy()
        
        # ì›ë³¸ í¬ê¸°ë¡œ í¬ë¡­
        disparity = disparity[:original_h, :original_w]
        
        return disparity


def demo_aanet():
    """AANet ë°ëª¨"""
    
    print("="*60)
    print("AANet ë°ëª¨")
    print("="*60)
    
    # ëª¨ë¸ ê²½ë¡œ
    model_path = "models/aanet_kitti15.pth"
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
    img_left = cv2.imread("test_left.png")
    img_right = cv2.imread("test_right.png")
    
    if img_left is None or img_right is None:
        print("í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¶”ë¡ 
    aanet = AANetInference(model_path, max_disp=192)
    
    import time
    start = time.time()
    disparity = aanet.inference(img_left, img_right)
    elapsed = time.time() - start
    
    print(f"\nì¶”ë¡  ì‹œê°„: {elapsed*1000:.1f} ms")
    print(f"ì‹œì°¨ ë²”ìœ„: {disparity.min():.1f} ~ {disparity.max():.1f}")
    
    # ì‹œê°í™”
    disp_vis = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX)
    disp_color = cv2.applyColorMap(disp_vis.astype(np.uint8), cv2.COLORMAP_MAGMA)
    
    cv2.imwrite("aanet_result.png", disp_color)
    print("âœ… ì €ì¥ë¨: aanet_result.png")


if __name__ == "__main__":
    demo_aanet()
```

---

## 6. ê¸°íƒ€ ì£¼ìš” ëª¨ë¸

### 6.1 ëª¨ë¸ ìš”ì•½

| ëª¨ë¸ | ì—°ë„ | íŠ¹ì§• | GitHub |
|------|------|------|--------|
| **PSMNet** | 2018 | í”¼ë¼ë¯¸ë“œ í’€ë§, 3D Conv | [JiaRenChang/PSMNet](https://github.com/JiaRenChang/PSMNet) |
| **GwcNet** | 2019 | ê·¸ë£¹ ìƒê´€, ê°œì„ ëœ ì§‘ê³„ | [xy-guo/GwcNet](https://github.com/xy-guo/GwcNet) |
| **GA-Net** | 2019 | ê°€ì´ë“œ ì§‘ê³„, SGA | [feihuzhang/GANet](https://github.com/feihuzhang/GANet) |
| **IGEV-Stereo** | 2023 | ê¸°í•˜í•™ ì¸ì½”ë”©, SOTA | [gangweiX/IGEV](https://github.com/gangweiX/IGEV) |
| **Unimatch** | 2023 | í†µí•© ë§¤ì¹­, ë²”ìš© | [autonomousvision/unimatch](https://github.com/autonomousvision/unimatch) |

### 6.2 ê°„ë‹¨í•œ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  "ìµœê³  ì •í™•ë„ê°€ í•„ìš”í•´"                                       â”‚
â”‚      â†’ RAFT-Stereo ë˜ëŠ” IGEV-Stereo                        â”‚
â”‚      â†’ ì¶”ë¡  ì‹œê°„: 200-400ms                                 â”‚
â”‚                                                             â”‚
â”‚  "ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ í•„ìš”í•´" (>10 FPS)                            â”‚
â”‚      â†’ AANet ë˜ëŠ” RAFT-Stereo (realtime)                   â”‚
â”‚      â†’ ì¶”ë¡  ì‹œê°„: 50-100ms                                  â”‚
â”‚                                                             â”‚
â”‚  "ë©”ëª¨ë¦¬ê°€ ì œí•œì ì´ì•¼" (<6GB VRAM)                           â”‚
â”‚      â†’ AANet ë˜ëŠ” PSMNet (ì‘ì€ í•´ìƒë„)                      â”‚
â”‚                                                             â”‚
â”‚  "ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ì ìš©í•  ê±°ì•¼"                                â”‚
â”‚      â†’ Unimatch (zero-shot ì¼ë°˜í™” ìš°ìˆ˜)                    â”‚
â”‚                                                             â”‚
â”‚  "ì…ë¬¸/í•™ìŠµ ëª©ì ì´ì•¼"                                        â”‚
â”‚      â†’ PSMNet (êµ¬ì¡° ì´í•´ ì‰¬ì›€, ìë£Œ ë§ìŒ)                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. ì „í†µì  ë°©ë²• vs ë”¥ëŸ¬ë‹ ë¹„êµ

### 7.1 ë¹„êµ ì½”ë“œ

```python
"""
compare_methods.py
ì „í†µì  ë°©ë²• vs ë”¥ëŸ¬ë‹ ë¹„êµ
"""

import cv2
import numpy as np
import time
import torch


class MethodComparison:
    """ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ ë°©ë²• ë¹„êµ"""
    
    def __init__(self):
        # SGBM
        self.sgbm = cv2.StereoSGBM_create(
            minDisparity=0,
            numDisparities=128,
            blockSize=5,
            P1=8 * 3 * 5 ** 2,
            P2=32 * 3 * 5 ** 2,
            disp12MaxDiff=1,
            uniquenessRatio=10,
            speckleWindowSize=100,
            speckleRange=2,
            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
        )
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ë³„ë„ ë¡œë“œ í•„ìš”
        self.raft_stereo = None
        self.aanet = None
    
    def run_sgbm(self, img_left, img_right):
        """SGBM ì‹¤í–‰"""
        
        start = time.time()
        disparity = self.sgbm.compute(img_left, img_right).astype(np.float32) / 16.0
        elapsed = time.time() - start
        
        return disparity, elapsed
    
    def run_raft_stereo(self, img_left, img_right, iters=32):
        """RAFT-Stereo ì‹¤í–‰"""
        
        if self.raft_stereo is None:
            print("RAFT-Stereo ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None, 0
        
        start = time.time()
        disparity = self.raft_stereo.inference(img_left, img_right, iters=iters)
        elapsed = time.time() - start
        
        return disparity, elapsed
    
    def run_aanet(self, img_left, img_right):
        """AANet ì‹¤í–‰"""
        
        if self.aanet is None:
            print("AANet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None, 0
        
        start = time.time()
        disparity = self.aanet.inference(img_left, img_right)
        elapsed = time.time() - start
        
        return disparity, elapsed
    
    def compute_metrics(self, pred, gt, max_disp=192):
        """
        í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Parameters:
        - pred: ì˜ˆì¸¡ ì‹œì°¨ ë§µ
        - gt: Ground Truth ì‹œì°¨ ë§µ
        
        Returns:
        - metrics: ë”•ì…”ë„ˆë¦¬
        """
        
        # ìœ íš¨í•œ ì˜ì—­ë§Œ
        mask = (gt > 0) & (gt < max_disp)
        
        if not np.any(mask):
            return {'valid': False}
        
        pred_valid = pred[mask]
        gt_valid = gt[mask]
        
        # ì˜¤ì°¨ ê³„ì‚°
        error = np.abs(pred_valid - gt_valid)
        
        # EPE (End-Point Error)
        epe = np.mean(error)
        
        # D1 (> 3px ë˜ëŠ” > 5% ì˜¤ì°¨ ë¹„ìœ¨)
        bad3 = np.mean((error > 3) & (error / gt_valid > 0.05)) * 100
        
        # RMSE
        rmse = np.sqrt(np.mean(error ** 2))
        
        return {
            'valid': True,
            'EPE': epe,
            'D1': bad3,
            'RMSE': rmse,
            'valid_ratio': np.sum(mask) / mask.size * 100
        }
    
    def compare_visual(self, img_left, results, save_path="comparison.png"):
        """
        ì‹œê°ì  ë¹„êµ
        
        Parameters:
        - img_left: ì™¼ìª½ ì´ë¯¸ì§€
        - results: {'method_name': (disparity, time), ...}
        """
        
        num_results = len(results)
        fig_height = 300
        fig_width = img_left.shape[1]
        
        # ì „ì²´ ì´ë¯¸ì§€
        canvas = np.zeros((fig_height * (num_results + 1), fig_width, 3), dtype=np.uint8)
        
        # ì›ë³¸ ì´ë¯¸ì§€
        canvas[:fig_height, :, :] = cv2.resize(img_left, (fig_width, fig_height))
        cv2.putText(canvas, "Input", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                   1, (255, 255, 255), 2)
        
        # ê° ë°©ë²•ì˜ ê²°ê³¼
        row = 1
        for name, (disparity, elapsed) in results.items():
            if disparity is None:
                continue
            
            # ì‹œê°í™”
            disp_vis = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX)
            disp_color = cv2.applyColorMap(disp_vis.astype(np.uint8), cv2.COLORMAP_MAGMA)
            disp_color = cv2.resize(disp_color, (fig_width, fig_height))
            
            y_start = row * fig_height
            canvas[y_start:y_start+fig_height, :, :] = disp_color
            
            # í…ìŠ¤íŠ¸
            cv2.putText(canvas, f"{name}: {elapsed*1000:.1f}ms", 
                       (10, y_start + 30), cv2.FONT_HERSHEY_SIMPLEX, 
                       1, (255, 255, 255), 2)
            
            row += 1
        
        cv2.imwrite(save_path, canvas)
        print(f"âœ… ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ë¨: {save_path}")
        
        return canvas


def demo_comparison():
    """ë¹„êµ ë°ëª¨"""
    
    print("="*60)
    print("ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ ë°©ë²• ë¹„êµ")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
    img_left = cv2.imread("test_left.png")
    img_right = cv2.imread("test_right.png")
    
    if img_left is None:
        print("í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì—†ìŒ. ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±...")
        img_left = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        img_right = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    
    comp = MethodComparison()
    
    results = {}
    
    # SGBM
    print("\n[1] SGBM ì‹¤í–‰...")
    disp_sgbm, time_sgbm = comp.run_sgbm(img_left, img_right)
    results['SGBM'] = (disp_sgbm, time_sgbm)
    print(f"    ì‹œê°„: {time_sgbm*1000:.1f} ms")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ê²°ê³¼ ìš”ì•½")
    print("="*60)
    print(f"{'ë°©ë²•':<15} {'ì‹œê°„ (ms)':<15} {'FPS':<10}")
    print("-"*40)
    
    for name, (disp, t) in results.items():
        if disp is not None:
            fps = 1.0 / t if t > 0 else 0
            print(f"{name:<15} {t*1000:<15.1f} {fps:<10.1f}")
    
    # ì‹œê°ì  ë¹„êµ
    comp.compare_visual(img_left, results)


if __name__ == "__main__":
    demo_comparison()
```

### 7.2 ë¹„êµ í‘œ

| í•­ëª© | SGBM | RAFT-Stereo | AANet |
|------|------|-------------|-------|
| **ì •í™•ë„ (D1)** | ~10% | ~1.3% | ~2% |
| **ì†ë„ (640x480)** | ~30ms | ~300ms | ~70ms |
| **ë©”ëª¨ë¦¬** | ~100MB | ~4GB | ~2GB |
| **í…ìŠ¤ì²˜ ì—†ëŠ” ì˜ì—­** | âŒ ì·¨ì•½ | âœ… ê°•í•¨ | âœ… ê°•í•¨ |
| **GPU í•„ìš”** | âŒ | âœ… | âœ… |
| **íŒŒë¼ë¯¸í„° íŠœë‹** | í•„ìš” | ë¶ˆí•„ìš” | ë¶ˆí•„ìš” |
| **ì¼ë°˜í™”** | ë„ë©”ì¸ ì˜ì¡´ | ìš°ìˆ˜ | ì–‘í˜¸ |

---

## 8. ì»¤ìŠ¤í…€ ë°ì´í„° ì ìš©

### 8.1 ì •ë¥˜ í›„ ë”¥ëŸ¬ë‹ ì ìš©

```python
"""
custom_data_pipeline.py
ì»¤ìŠ¤í…€ ë°ì´í„°ì— ë”¥ëŸ¬ë‹ ëª¨ë¸ ì ìš©
"""

import cv2
import numpy as np
import yaml


class CustomStereoPipeline:
    """ì»¤ìŠ¤í…€ ìŠ¤í…Œë ˆì˜¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, calibration_file, model_type='sgbm'):
        """
        Parameters:
        - calibration_file: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒì¼ ê²½ë¡œ
        - model_type: 'sgbm', 'raft', 'aanet'
        """
        
        self.load_calibration(calibration_file)
        self.model_type = model_type
        self.model = None
        
        # SGBMì€ ê¸°ë³¸ ìƒì„±
        if model_type == 'sgbm':
            self.model = cv2.StereoSGBM_create(
                minDisparity=0,
                numDisparities=128,
                blockSize=5,
                P1=8 * 3 * 5 ** 2,
                P2=32 * 3 * 5 ** 2,
                mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
            )
    
    def load_calibration(self, filename):
        """ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¡œë“œ"""
        
        with open(filename, 'r') as f:
            params = yaml.safe_load(f)
        
        self.img_size = tuple(params['image_size'])
        self.K1 = np.array(params['K1'])
        self.D1 = np.array(params['D1'])
        self.K2 = np.array(params['K2'])
        self.D2 = np.array(params['D2'])
        self.R1 = np.array(params['R1'])
        self.R2 = np.array(params['R2'])
        self.P1 = np.array(params['P1'])
        self.P2 = np.array(params['P2'])
        self.Q = np.array(params['Q'])
        self.baseline = params['baseline_mm']
        
        # ì •ë¥˜ ë§µ ìƒì„±
        self.map1_left, self.map2_left = cv2.initUndistortRectifyMap(
            self.K1, self.D1, self.R1, self.P1, self.img_size, cv2.CV_32FC1
        )
        self.map1_right, self.map2_right = cv2.initUndistortRectifyMap(
            self.K2, self.D2, self.R2, self.P2, self.img_size, cv2.CV_32FC1
        )
        
        print(f"âœ… ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¡œë“œ: {self.img_size}")
    
    def load_dl_model(self, model_path):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ"""
        
        if self.model_type == 'raft':
            from raft_stereo_inference import RAFTStereoInference
            self.model = RAFTStereoInference(model_path)
        elif self.model_type == 'aanet':
            from aanet_inference import AANetInference
            self.model = AANetInference(model_path)
    
    def rectify(self, img_left, img_right):
        """ì´ë¯¸ì§€ ì •ë¥˜"""
        
        rect_left = cv2.remap(img_left, self.map1_left, self.map2_left, cv2.INTER_LINEAR)
        rect_right = cv2.remap(img_right, self.map1_right, self.map2_right, cv2.INTER_LINEAR)
        
        return rect_left, rect_right
    
    def compute_disparity(self, rect_left, rect_right):
        """ì‹œì°¨ ê³„ì‚°"""
        
        if self.model_type == 'sgbm':
            disparity = self.model.compute(rect_left, rect_right).astype(np.float32) / 16.0
        elif self.model_type in ['raft', 'aanet']:
            disparity = self.model.inference(rect_left, rect_right)
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
        
        return disparity
    
    def disparity_to_depth(self, disparity):
        """ì‹œì°¨ë¥¼ ê¹Šì´ë¡œ ë³€í™˜"""
        
        focal = self.P1[0, 0]
        depth = np.zeros_like(disparity)
        valid = disparity > 0
        depth[valid] = (focal * self.baseline) / disparity[valid]
        
        return depth
    
    def process(self, img_left, img_right):
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Returns:
        - rect_left: ì •ë¥˜ëœ ì™¼ìª½ ì´ë¯¸ì§€
        - disparity: ì‹œì°¨ ë§µ
        - depth: ê¹Šì´ ë§µ (mm)
        """
        
        # 1. ì •ë¥˜
        rect_left, rect_right = self.rectify(img_left, img_right)
        
        # 2. ì‹œì°¨ ê³„ì‚°
        disparity = self.compute_disparity(rect_left, rect_right)
        
        # 3. ê¹Šì´ ë³€í™˜
        depth = self.disparity_to_depth(disparity)
        
        return rect_left, disparity, depth


def demo_custom_pipeline():
    """ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸ ë°ëª¨"""
    
    print("="*60)
    print("ì»¤ìŠ¤í…€ ë°ì´í„° íŒŒì´í”„ë¼ì¸")
    print("="*60)
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„± (SGBM)
    pipeline = CustomStereoPipeline("stereo_params.yaml", model_type='sgbm')
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    img_left = cv2.imread("test_left.png")
    img_right = cv2.imread("test_right.png")
    
    if img_left is None:
        print("í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì²˜ë¦¬
    rect_left, disparity, depth = pipeline.process(img_left, img_right)
    
    print(f"\nê²°ê³¼:")
    print(f"  ì‹œì°¨ ë²”ìœ„: {disparity[disparity > 0].min():.1f} ~ {disparity.max():.1f} px")
    print(f"  ê¹Šì´ ë²”ìœ„: {depth[depth > 0].min():.1f} ~ {depth[depth > 0].max():.1f} mm")
    
    # ì‹œê°í™”
    disp_vis = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX)
    disp_color = cv2.applyColorMap(disp_vis.astype(np.uint8), cv2.COLORMAP_MAGMA)
    
    cv2.imwrite("custom_disparity.png", disp_color)
    print("âœ… ì €ì¥ë¨: custom_disparity.png")


if __name__ == "__main__":
    demo_custom_pipeline()
```

---

## 9. ì„±ëŠ¥ ìµœì í™”

### 9.1 ì¶”ë¡  ì†ë„ ìµœì í™”

```python
"""
optimization.py
ë”¥ëŸ¬ë‹ ëª¨ë¸ ìµœì í™”
"""

import torch
import numpy as np
import time


def optimize_with_half_precision(model):
    """
    FP16 (Half Precision) ë³€í™˜
    
    - ë©”ëª¨ë¦¬ ~50% ê°ì†Œ
    - ì†ë„ ~2x í–¥ìƒ (ìµœì‹  GPU)
    """
    
    model = model.half()
    print("âœ… FP16 ëª¨ë“œ í™œì„±í™”")
    
    return model


def optimize_with_torch_compile(model):
    """
    PyTorch 2.0 torch.compile ì‚¬ìš©
    
    - ìë™ ìµœì í™”
    - ì†ë„ ~1.5-2x í–¥ìƒ
    """
    
    if hasattr(torch, 'compile'):
        model = torch.compile(model, mode='reduce-overhead')
        print("âœ… torch.compile í™œì„±í™”")
    else:
        print("âš ï¸ torch.compile ë¯¸ì§€ì› (PyTorch 2.0+ í•„ìš”)")
    
    return model


def optimize_with_tensorrt(model, input_shape, save_path="model.engine"):
    """
    TensorRT ë³€í™˜ (NVIDIA GPU ì „ìš©)
    
    - ì†ë„ 2-5x í–¥ìƒ
    - ì¶”ë¡  ì „ìš©
    """
    
    try:
        import torch_tensorrt
        
        # TensorRT ì»´íŒŒì¼
        inputs = [torch_tensorrt.Input(shape=input_shape, dtype=torch.float32)]
        
        trt_model = torch_tensorrt.compile(
            model,
            inputs=inputs,
            enabled_precisions={torch.float32, torch.float16},
            workspace_size=1 << 30  # 1GB
        )
        
        torch.jit.save(trt_model, save_path)
        print(f"âœ… TensorRT ëª¨ë¸ ì €ì¥: {save_path}")
        
        return trt_model
    
    except ImportError:
        print("âš ï¸ torch_tensorrt ë¯¸ì„¤ì¹˜")
        return model


def benchmark_inference(model, input_left, input_right, num_runs=50, warmup=10):
    """
    ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
    
    Parameters:
    - model: ëª¨ë¸
    - input_left, input_right: ì…ë ¥ í…ì„œ
    - num_runs: ì¸¡ì • íšŸìˆ˜
    - warmup: ì›Œë°ì—… íšŸìˆ˜
    """
    
    device = next(model.parameters()).device
    
    # GPU ë™ê¸°í™” í•¨ìˆ˜
    if device.type == 'cuda':
        sync = torch.cuda.synchronize
    else:
        sync = lambda: None
    
    # ì›Œë°ì—…
    print("ì›Œë°ì—… ì¤‘...")
    with torch.no_grad():
        for _ in range(warmup):
            _ = model(input_left, input_right)
            sync()
    
    # ë²¤ì¹˜ë§ˆí¬
    print(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ({num_runs}íšŒ)...")
    times = []
    
    with torch.no_grad():
        for _ in range(num_runs):
            sync()
            start = time.perf_counter()
            
            _ = model(input_left, input_right)
            
            sync()
            elapsed = time.perf_counter() - start
            times.append(elapsed)
    
    times = np.array(times)
    
    print("\n" + "="*50)
    print("ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
    print("="*50)
    print(f"í‰ê·  ì‹œê°„: {times.mean()*1000:.2f} ms")
    print(f"í‘œì¤€í¸ì°¨: {times.std()*1000:.2f} ms")
    print(f"ìµœì†Œ ì‹œê°„: {times.min()*1000:.2f} ms")
    print(f"ìµœëŒ€ ì‹œê°„: {times.max()*1000:.2f} ms")
    print(f"FPS: {1/times.mean():.1f}")
    
    return times
```

### 9.2 í•´ìƒë„ë³„ ì†ë„ ê°€ì´ë“œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              í•´ìƒë„ë³„ ì¶”ë¡  ì†ë„ ê°€ì´ë“œ (RTX 3080 ê¸°ì¤€)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  í•´ìƒë„        RAFT-Stereo    AANet      ê¶Œì¥ ìš©ë„          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  320Ã—240       ~50ms          ~20ms      ì‹¤ì‹œê°„ ì„ë² ë””ë“œ     â”‚
â”‚  640Ã—480       ~150ms         ~50ms      ì‹¤ì‹œê°„ PC          â”‚
â”‚  1280Ã—720      ~400ms         ~120ms     ì¤€ì‹¤ì‹œê°„           â”‚
â”‚  1920Ã—1080     ~800ms         ~250ms     ì˜¤í”„ë¼ì¸ ì²˜ë¦¬      â”‚
â”‚                                                             â”‚
â”‚  * iters=12 (RAFT-Stereo ì‹¤ì‹œê°„ ëª¨ë“œ)                       â”‚
â”‚  * FP16 ëª¨ë“œ ì‚¬ìš© ì‹œ ~30% ì¶”ê°€ í–¥ìƒ                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. ì‹¤ìŠµ í”„ë¡œì íŠ¸

### 10.1 ì¢…í•© ë¹„êµ ì‹œìŠ¤í…œ

```python
"""
stereo_dl_demo.py
ë”¥ëŸ¬ë‹ ìŠ¤í…Œë ˆì˜¤ ì¢…í•© ë°ëª¨
"""

import cv2
import numpy as np
import time
import argparse


def main():
    parser = argparse.ArgumentParser(description='ë”¥ëŸ¬ë‹ ìŠ¤í…Œë ˆì˜¤ ë°ëª¨')
    parser.add_argument('--left', type=str, required=True, help='ì™¼ìª½ ì´ë¯¸ì§€')
    parser.add_argument('--right', type=str, required=True, help='ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€')
    parser.add_argument('--calibration', type=str, default=None, help='ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒì¼')
    parser.add_argument('--method', type=str, default='sgbm', 
                       choices=['sgbm', 'raft', 'aanet', 'all'])
    parser.add_argument('--output', type=str, default='result.png')
    args = parser.parse_args()
    
    print("="*60)
    print("ë”¥ëŸ¬ë‹ ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ ë°ëª¨")
    print("="*60)
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    img_left = cv2.imread(args.left)
    img_right = cv2.imread(args.right)
    
    if img_left is None or img_right is None:
        print("âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    print(f"ì´ë¯¸ì§€ í¬ê¸°: {img_left.shape}")
    
    # ì •ë¥˜ (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ìˆëŠ” ê²½ìš°)
    if args.calibration:
        from custom_data_pipeline import CustomStereoPipeline
        pipeline = CustomStereoPipeline(args.calibration, model_type='sgbm')
        img_left, img_right = pipeline.rectify(img_left, img_right)
        print("âœ… ì •ë¥˜ ì™„ë£Œ")
    
    results = {}
    
    # SGBM
    if args.method in ['sgbm', 'all']:
        print("\n[SGBM] ì‹¤í–‰ ì¤‘...")
        sgbm = cv2.StereoSGBM_create(
            minDisparity=0, numDisparities=128, blockSize=5,
            P1=8*3*5**2, P2=32*3*5**2,
            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
        )
        
        start = time.time()
        disp_sgbm = sgbm.compute(img_left, img_right).astype(np.float32) / 16.0
        time_sgbm = time.time() - start
        
        results['SGBM'] = (disp_sgbm, time_sgbm)
        print(f"  ì‹œê°„: {time_sgbm*1000:.1f}ms, FPS: {1/time_sgbm:.1f}")
    
    # ê²°ê³¼ ì‹œê°í™”
    num_results = len(results)
    h, w = img_left.shape[:2]
    
    canvas = np.zeros((h * 2, w * max(1, num_results), 3), dtype=np.uint8)
    
    # ì›ë³¸ ì´ë¯¸ì§€
    canvas[:h, :w] = img_left
    cv2.putText(canvas, "Input (Rectified)", (10, 30),
               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # ê²°ê³¼
    col = 0
    for name, (disp, t) in results.items():
        disp_vis = cv2.normalize(disp, None, 0, 255, cv2.NORM_MINMAX)
        disp_vis[disp <= 0] = 0
        disp_color = cv2.applyColorMap(disp_vis.astype(np.uint8), cv2.COLORMAP_MAGMA)
        
        x_start = col * w
        canvas[h:, x_start:x_start+w] = disp_color
        
        cv2.putText(canvas, f"{name}: {t*1000:.1f}ms", (x_start+10, h+30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        col += 1
    
    cv2.imwrite(args.output, canvas)
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {args.output}")
    
    # ë””ìŠ¤í”Œë ˆì´
    cv2.imshow("Result", cv2.resize(canvas, (canvas.shape[1]//2, canvas.shape[0]//2)))
    cv2.waitKey(0)
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
```

---

## ğŸ“ í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì´ë¡  ì´í•´

- [ ] Cost Volumeì˜ ê°œë…ê³¼ êµ¬ì„± ë°©ë²•ì„ ì´í•´í–ˆë‹¤
- [ ] Soft Argminì˜ ì›ë¦¬ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] RAFT-Stereoì˜ ë°˜ë³µì  ì—…ë°ì´íŠ¸ êµ¬ì¡°ë¥¼ ì´í•´í–ˆë‹¤
- [ ] AANetì˜ ì ì‘í˜• ì§‘ê³„ ê°œë…ì„ ì•Œê³  ìˆë‹¤
- [ ] ì „í†µì  ë°©ë²•ê³¼ ë”¥ëŸ¬ë‹ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤

### ì‹¤ìŠµ ì™„ë£Œ

- [ ] ë”¥ëŸ¬ë‹ í™˜ê²½ ì„¤ì • (PyTorch, CUDA)
- [ ] RAFT-Stereo ë˜ëŠ” AANet ì¶”ë¡  ì‹¤í–‰
- [ ] ì „í†µì  ë°©ë²• (SGBM)ê³¼ ë¹„êµ
- [ ] ì»¤ìŠ¤í…€ ë°ì´í„°ì— ì ìš©
- [ ] ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬

---

## â¡ï¸ ë‹¤ìŒ ëª¨ë“ˆ

**[Module 06: ROS2 ì—°ë™ ê°œë°œ](../Module_06_ROS2/README.md)**

ë‹¤ìŒ ëª¨ë“ˆì—ì„œëŠ”:
- ROS2 ê¸°ì´ˆ ë° ì„¤ì¹˜
- ìŠ¤í…Œë ˆì˜¤ ì¹´ë©”ë¼ ROS2 ë…¸ë“œ ê°œë°œ
- ê¹Šì´ ì´ë¯¸ì§€ í¼ë¸”ë¦¬ì‹œ
- í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í¼ë¸”ë¦¬ì‹œ
- Rviz2 ì‹œê°í™”

ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ ì‚¬ìš©, ìˆ˜ì •, ë°°í¬ ê°€ëŠ¥
