# Project 04: Visual Odometry (ì‹œê°ì  ì£¼í–‰ ê±°ë¦¬ ì¸¡ì •)

[![ë‚œì´ë„](https://img.shields.io/badge/ë‚œì´ë„-â­â­â­â­â­_ì „ë¬¸ê°€-purple.svg)]()
[![ì˜ˆìƒì‹œê°„](https://img.shields.io/badge/ì˜ˆìƒì‹œê°„-15--20ì‹œê°„-blue.svg)]()
[![ì„ ìˆ˜ì§€ì‹](https://img.shields.io/badge/ì„ ìˆ˜ì§€ì‹-Module_01--04,_ì„ í˜•ëŒ€ìˆ˜-orange.svg)]()

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ëª©í‘œ** | ìŠ¤í…Œë ˆì˜¤ ì¹´ë©”ë¼ë¡œ ì¹´ë©”ë¼/ë¡œë´‡ì˜ ì´ë™ ê²½ë¡œ ì¶”ì • |
| **ê¸°ëŠ¥** | íŠ¹ì§•ì  ì¶”ì , ëª¨ì…˜ ì¶”ì •, 3D ê¶¤ì  ì‹œê°í™”, ë§µ ìƒì„± |
| **ì‘ìš©** | ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜, ë“œë¡  ììœ¨ë¹„í–‰, AR/VR, SLAM |

---

## ğŸ“‹ ëª©ì°¨

1. [Visual Odometry ê°œìš”](#1-visual-odometry-ê°œìš”)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#2-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [íŠ¹ì§•ì  ê²€ì¶œ ë° ë§¤ì¹­](#3-íŠ¹ì§•ì -ê²€ì¶œ-ë°-ë§¤ì¹­)
4. [ìŠ¤í…Œë ˆì˜¤ 3D ì¬êµ¬ì„±](#4-ìŠ¤í…Œë ˆì˜¤-3d-ì¬êµ¬ì„±)
5. [ëª¨ì…˜ ì¶”ì •](#5-ëª¨ì…˜-ì¶”ì •)
6. [ê¶¤ì  ì¶”ì ](#6-ê¶¤ì -ì¶”ì )
7. [ìµœì í™” ë° ë“œë¦¬í”„íŠ¸ ë³´ì •](#7-ìµœì í™”-ë°-ë“œë¦¬í”„íŠ¸-ë³´ì •)
8. [ì‹œê°í™”](#8-ì‹œê°í™”)
9. [ì „ì²´ ì½”ë“œ](#9-ì „ì²´-ì½”ë“œ)
10. [ì„±ëŠ¥ í‰ê°€](#10-ì„±ëŠ¥-í‰ê°€)

## ğŸ“‹ Project 04 ì£¼ìš” ë‚´ìš©

| ì„¹ì…˜ | ë‚´ìš© |
|------|------|
| 1. Visual Odometry ê°œìš” | VO ê°œë…, Mono vs Stereo, íŒŒì´í”„ë¼ì¸ |
| 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ | í”„ë¡œì íŠ¸ êµ¬ì¡° |
| 3. íŠ¹ì§•ì  ê²€ì¶œ ë° ë§¤ì¹­ | FeatureDetector, FeatureTracker, FeatureMatcher |
| 4. ìŠ¤í…Œë ˆì˜¤ 3D ì¬êµ¬ì„± | StereoTriangulator, ì‚¼ê°ì¸¡ëŸ‰ |
| 5. ëª¨ì…˜ ì¶”ì • | MotionEstimator (PnP, Essential Matrix), RANSAC |
| 6. ê¶¤ì  ì¶”ì  | TrajectoryTracker, í¬ì¦ˆ ëˆ„ì  |
| 7. ìµœì í™” ë° ë“œë¦¬í”„íŠ¸ ë³´ì • | Bundle Adjustment, Loop Closure |
| 8. ì‹œê°í™” | VOVisualizer (2D/3D ê¶¤ì ) |
| 9. ì „ì²´ ì½”ë“œ | StereoVisualOdometry í´ë˜ìŠ¤, main.py |
| 10. ì„±ëŠ¥ í‰ê°€ | ATE, RPE ë©”íŠ¸ë¦­ |


---

## 1. Visual Odometry ê°œìš”

### 1.1 Visual Odometryë€?

Visual Odometry(VO)ëŠ” ì—°ì†ì ì¸ ì¹´ë©”ë¼ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì¹´ë©”ë¼ì˜ ìœ„ì¹˜ì™€ ë°©í–¥ ë³€í™”ë¥¼ ì¶”ì •í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Visual Odometry ê°œë…                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   ì‹œê°„ t         ì‹œê°„ t+1       ì‹œê°„ t+2                     â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”          â”Œâ”€â”€â”€â”          â”Œâ”€â”€â”€â”                      â”‚
â”‚   â”‚ ğŸ“· â”‚ â”€â”€â”€â”€â”€â”€â†’ â”‚ ğŸ“· â”‚ â”€â”€â”€â”€â”€â”€â†’ â”‚ ğŸ“· â”‚                      â”‚
â”‚   â””â”€â”€â”€â”˜          â””â”€â”€â”€â”˜          â””â”€â”€â”€â”˜                      â”‚
â”‚     Pâ‚€             Pâ‚             Pâ‚‚                        â”‚
â”‚                                                             â”‚
â”‚   ì´ë¯¸ì§€ ë¶„ì„ì„ í†µí•´ ì¹´ë©”ë¼ ì´ë™ (Tâ‚€â‚, Tâ‚â‚‚) ì¶”ì •            â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚                                         â”‚              â”‚
â”‚   â”‚    Pâ‚€ â”€â”€Tâ‚€â‚â”€â”€â†’ Pâ‚ â”€â”€Tâ‚â‚‚â”€â”€â†’ Pâ‚‚          â”‚              â”‚
â”‚   â”‚     â†“                                   â”‚              â”‚
â”‚   â”‚   ëˆ„ì í•˜ì—¬ ì „ì²´ ê¶¤ì  ìƒì„±               â”‚              â”‚
â”‚   â”‚                                         â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Monocular vs Stereo VO

| íŠ¹ì„± | Monocular VO | Stereo VO |
|------|-------------|-----------|
| **ì¹´ë©”ë¼** | 1ëŒ€ | 2ëŒ€ |
| **ìŠ¤ì¼€ì¼** | ì•Œ ìˆ˜ ì—†ìŒ (up-to-scale) | ì‹¤ì œ ìŠ¤ì¼€ì¼ ë³µì› ê°€ëŠ¥ |
| **ì´ˆê¸°í™”** | í•„ìš” (5-point ë“±) | ë¶ˆí•„ìš” |
| **ì •í™•ë„** | ìƒëŒ€ì  ë‚®ìŒ | ë†’ìŒ |
| **ë¹„ìš©** | ì €ë ´ | ìƒëŒ€ì  ë†’ìŒ |

### 1.3 VO íŒŒì´í”„ë¼ì¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Stereo Visual Odometry íŒŒì´í”„ë¼ì¸               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  [Frame t-1]         [Frame t]                              â”‚
â”‚  Left  Right         Left  Right                            â”‚
â”‚    â”‚     â”‚             â”‚     â”‚                              â”‚
â”‚    â””â”€â”€â”¬â”€â”€â”˜             â””â”€â”€â”¬â”€â”€â”˜                              â”‚
â”‚       â”‚                   â”‚                                 â”‚
â”‚       â–¼                   â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚Feature  â”‚         â”‚Feature  â”‚                           â”‚
â”‚  â”‚Detectionâ”‚         â”‚Detectionâ”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚       â”‚                   â”‚                                 â”‚
â”‚       â–¼                   â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ Stereo  â”‚         â”‚ Stereo  â”‚                           â”‚
â”‚  â”‚Matching â”‚         â”‚Matching â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚       â”‚                   â”‚                                 â”‚
â”‚       â–¼                   â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚3D Pointsâ”‚         â”‚3D Pointsâ”‚                           â”‚
â”‚  â”‚  (t-1)  â”‚         â”‚   (t)   â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                           â”‚
â”‚       â”‚                   â”‚                                 â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                 â–¼                                           â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚         â”‚  Temporal   â”‚                                    â”‚
â”‚         â”‚  Matching   â”‚                                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                â–¼                                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚         â”‚   Motion    â”‚                                    â”‚
â”‚         â”‚ Estimation  â”‚                                    â”‚
â”‚         â”‚  (PnP/ICP)  â”‚                                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                â–¼                                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚         â”‚    Pose     â”‚                                    â”‚
â”‚         â”‚   Update    â”‚                                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Project_04_Visual_Odometry/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ stereo_params.yaml      # ìº˜ë¦¬ë¸Œë ˆì´ì…˜
â”‚   â””â”€â”€ vo_config.yaml          # VO ì„¤ì •
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # ë©”ì¸ ì‹¤í–‰
â”‚   â”œâ”€â”€ stereo_camera.py        # ìŠ¤í…Œë ˆì˜¤ ì¹´ë©”ë¼
â”‚   â”œâ”€â”€ feature_detector.py     # íŠ¹ì§•ì  ê²€ì¶œ
â”‚   â”œâ”€â”€ feature_matcher.py      # íŠ¹ì§•ì  ë§¤ì¹­
â”‚   â”œâ”€â”€ stereo_matcher.py       # ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­
â”‚   â”œâ”€â”€ motion_estimator.py     # ëª¨ì…˜ ì¶”ì •
â”‚   â”œâ”€â”€ trajectory_tracker.py   # ê¶¤ì  ì¶”ì 
â”‚   â”œâ”€â”€ local_map.py            # ë¡œì»¬ ë§µ
â”‚   â”œâ”€â”€ visualizer.py           # ì‹œê°í™”
â”‚   â””â”€â”€ utils.py                # ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sequences/              # í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤
â””â”€â”€ output/
    â”œâ”€â”€ trajectories/           # ê¶¤ì  ê²°ê³¼
    â””â”€â”€ maps/                   # ë§µ ì¶œë ¥
```

---

## 3. íŠ¹ì§•ì  ê²€ì¶œ ë° ë§¤ì¹­

### 3.1 íŠ¹ì§•ì  ê²€ì¶œê¸°

```python
"""
feature_detector.py
íŠ¹ì§•ì  ê²€ì¶œ
"""

import cv2
import numpy as np
from typing import Tuple, List, Optional
from enum import Enum


class DetectorType(Enum):
    """ê²€ì¶œê¸° íƒ€ì…"""
    ORB = 1
    SIFT = 2
    SURF = 3
    FAST = 4
    GFTT = 5  # Good Features To Track


class FeatureDetector:
    """íŠ¹ì§•ì  ê²€ì¶œê¸°"""
    
    def __init__(self, detector_type: DetectorType = DetectorType.ORB,
                 max_features: int = 2000):
        """
        Parameters:
        - detector_type: ê²€ì¶œê¸° íƒ€ì…
        - max_features: ìµœëŒ€ íŠ¹ì§•ì  ìˆ˜
        """
        
        self.detector_type = detector_type
        self.max_features = max_features
        
        self._create_detector()
    
    def _create_detector(self):
        """ê²€ì¶œê¸° ìƒì„±"""
        
        if self.detector_type == DetectorType.ORB:
            self.detector = cv2.ORB_create(
                nfeatures=self.max_features,
                scaleFactor=1.2,
                nlevels=8,
                edgeThreshold=31,
                firstLevel=0,
                WTA_K=2,
                patchSize=31,
                fastThreshold=20
            )
            self.descriptor = self.detector
            
        elif self.detector_type == DetectorType.SIFT:
            self.detector = cv2.SIFT_create(
                nfeatures=self.max_features,
                nOctaveLayers=3,
                contrastThreshold=0.04,
                edgeThreshold=10,
                sigma=1.6
            )
            self.descriptor = self.detector
            
        elif self.detector_type == DetectorType.FAST:
            self.detector = cv2.FastFeatureDetector_create(
                threshold=20,
                nonmaxSuppression=True
            )
            # FASTëŠ” ë””ìŠ¤í¬ë¦½í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ORB ì‚¬ìš©
            self.descriptor = cv2.ORB_create(nfeatures=self.max_features)
            
        elif self.detector_type == DetectorType.GFTT:
            self.detector = None  # cv2.goodFeaturesToTrack ì‚¬ìš©
            self.descriptor = cv2.ORB_create(nfeatures=self.max_features)
    
    def detect(self, image: np.ndarray, 
               mask: Optional[np.ndarray] = None) -> Tuple[List, np.ndarray]:
        """
        íŠ¹ì§•ì  ê²€ì¶œ ë° ë””ìŠ¤í¬ë¦½í„° ê³„ì‚°
        
        Parameters:
        - image: ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€
        - mask: ê´€ì‹¬ ì˜ì—­ ë§ˆìŠ¤í¬
        
        Returns:
        - keypoints: í‚¤í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        - descriptors: ë””ìŠ¤í¬ë¦½í„° ë°°ì—´
        """
        
        if len(image.shape) == 3:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        if self.detector_type == DetectorType.GFTT:
            # Good Features To Track
            corners = cv2.goodFeaturesToTrack(
                image,
                maxCorners=self.max_features,
                qualityLevel=0.01,
                minDistance=10,
                mask=mask
            )
            
            if corners is None:
                return [], None
            
            # KeyPointë¡œ ë³€í™˜
            keypoints = [cv2.KeyPoint(x=c[0][0], y=c[0][1], size=20) 
                        for c in corners]
            
            # ë””ìŠ¤í¬ë¦½í„° ê³„ì‚°
            keypoints, descriptors = self.descriptor.compute(image, keypoints)
        else:
            # ì¼ë°˜ ê²€ì¶œê¸°
            keypoints, descriptors = self.detector.detectAndCompute(image, mask)
        
        return keypoints, descriptors
    
    def detect_and_compute_stereo(self, left: np.ndarray, 
                                   right: np.ndarray) -> dict:
        """
        ìŠ¤í…Œë ˆì˜¤ ì´ë¯¸ì§€ ìŒì—ì„œ íŠ¹ì§•ì  ê²€ì¶œ
        
        Returns:
        - dict: {'left_kp', 'left_desc', 'right_kp', 'right_desc'}
        """
        
        left_kp, left_desc = self.detect(left)
        right_kp, right_desc = self.detect(right)
        
        return {
            'left_kp': left_kp,
            'left_desc': left_desc,
            'right_kp': right_kp,
            'right_desc': right_desc
        }


class FeatureTracker:
    """ê´‘í•™ íë¦„ ê¸°ë°˜ íŠ¹ì§•ì  ì¶”ì """
    
    def __init__(self, max_features: int = 2000):
        self.max_features = max_features
        
        # Lucas-Kanade íŒŒë¼ë¯¸í„°
        self.lk_params = dict(
            winSize=(21, 21),
            maxLevel=3,
            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01)
        )
        
        # íŠ¹ì§•ì  ê²€ì¶œ íŒŒë¼ë¯¸í„°
        self.feature_params = dict(
            maxCorners=max_features,
            qualityLevel=0.01,
            minDistance=10,
            blockSize=7
        )
        
        self.prev_image = None
        self.prev_points = None
        self.track_ids = None
        self.next_id = 0
    
    def initialize(self, image: np.ndarray) -> np.ndarray:
        """ì²« í”„ë ˆì„ ì´ˆê¸°í™”"""
        
        if len(image.shape) == 3:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        self.prev_image = image
        
        # íŠ¹ì§•ì  ê²€ì¶œ
        points = cv2.goodFeaturesToTrack(image, **self.feature_params)
        
        if points is not None:
            self.prev_points = points.reshape(-1, 2)
            self.track_ids = np.arange(len(self.prev_points))
            self.next_id = len(self.prev_points)
        else:
            self.prev_points = np.array([])
            self.track_ids = np.array([])
        
        return self.prev_points
    
    def track(self, image: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        í”„ë ˆì„ ê°„ íŠ¹ì§•ì  ì¶”ì 
        
        Returns:
        - prev_points: ì´ì „ í”„ë ˆì„ ì ë“¤
        - curr_points: í˜„ì¬ í”„ë ˆì„ ì ë“¤
        - track_ids: ì¶”ì  ID
        """
        
        if len(image.shape) == 3:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        if self.prev_points is None or len(self.prev_points) == 0:
            self.initialize(image)
            return None, None, None
        
        # ê´‘í•™ íë¦„ ì¶”ì 
        curr_points, status, _ = cv2.calcOpticalFlowPyrLK(
            self.prev_image, image,
            self.prev_points.astype(np.float32),
            None, **self.lk_params
        )
        
        # ì—­ë°©í–¥ ê²€ì¦
        if curr_points is not None:
            prev_points_back, status_back, _ = cv2.calcOpticalFlowPyrLK(
                image, self.prev_image,
                curr_points,
                None, **self.lk_params
            )
            
            # ì–‘ë°©í–¥ ì˜¤ì°¨
            fb_error = np.linalg.norm(
                self.prev_points - prev_points_back.reshape(-1, 2), axis=1
            )
            
            # ìœ íš¨í•œ ì¶”ì ë§Œ ì„ íƒ
            valid = (status.flatten() == 1) & (fb_error < 1.0)
        else:
            valid = np.zeros(len(self.prev_points), dtype=bool)
        
        # ìœ íš¨í•œ ì  í•„í„°ë§
        prev_valid = self.prev_points[valid]
        curr_valid = curr_points.reshape(-1, 2)[valid]
        ids_valid = self.track_ids[valid]
        
        # íŠ¹ì§•ì  ë³´ì¶©
        if len(curr_valid) < self.max_features * 0.5:
            mask = np.ones(image.shape[:2], dtype=np.uint8) * 255
            
            for pt in curr_valid:
                cv2.circle(mask, (int(pt[0]), int(pt[1])), 10, 0, -1)
            
            new_points = cv2.goodFeaturesToTrack(
                image, 
                maxCorners=self.max_features - len(curr_valid),
                qualityLevel=0.01,
                minDistance=10,
                mask=mask
            )
            
            if new_points is not None:
                new_points = new_points.reshape(-1, 2)
                new_ids = np.arange(self.next_id, self.next_id + len(new_points))
                self.next_id += len(new_points)
                
                curr_valid = np.vstack([curr_valid, new_points])
                ids_valid = np.concatenate([ids_valid, new_ids])
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        result_prev = self.prev_points[valid] if len(self.prev_points) > 0 else None
        
        self.prev_image = image
        self.prev_points = curr_valid
        self.track_ids = ids_valid
        
        return result_prev, curr_valid[:len(result_prev)] if result_prev is not None else None, ids_valid[:len(result_prev)] if result_prev is not None else None
```

### 3.2 íŠ¹ì§•ì  ë§¤ì¹­

```python
"""
feature_matcher.py
íŠ¹ì§•ì  ë§¤ì¹­
"""

import cv2
import numpy as np
from typing import List, Tuple, Optional


class FeatureMatcher:
    """íŠ¹ì§•ì  ë§¤ì¹­"""
    
    def __init__(self, matcher_type: str = 'BF', 
                 cross_check: bool = True,
                 ratio_threshold: float = 0.75):
        """
        Parameters:
        - matcher_type: 'BF' (Brute Force) ë˜ëŠ” 'FLANN'
        - cross_check: êµì°¨ ê²€ì¦ ì‚¬ìš©
        - ratio_threshold: Lowe's ratio test ì„ê³„ê°’
        """
        
        self.ratio_threshold = ratio_threshold
        
        if matcher_type == 'BF':
            self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=cross_check)
        else:
            # FLANN
            index_params = dict(algorithm=6,  # FLANN_INDEX_LSH
                               table_number=6,
                               key_size=12,
                               multi_probe_level=1)
            search_params = dict(checks=50)
            self.matcher = cv2.FlannBasedMatcher(index_params, search_params)
        
        self.cross_check = cross_check
    
    def match(self, desc1: np.ndarray, desc2: np.ndarray) -> List:
        """
        ë””ìŠ¤í¬ë¦½í„° ë§¤ì¹­
        
        Returns:
        - matches: ë§¤ì¹­ ë¦¬ìŠ¤íŠ¸
        """
        
        if desc1 is None or desc2 is None:
            return []
        
        if len(desc1) == 0 or len(desc2) == 0:
            return []
        
        if self.cross_check:
            matches = self.matcher.match(desc1, desc2)
            # ê±°ë¦¬ìˆœ ì •ë ¬
            matches = sorted(matches, key=lambda x: x.distance)
        else:
            # KNN ë§¤ì¹­ + Lowe's ratio test
            matches_knn = self.matcher.knnMatch(desc1, desc2, k=2)
            
            matches = []
            for m_n in matches_knn:
                if len(m_n) == 2:
                    m, n = m_n
                    if m.distance < self.ratio_threshold * n.distance:
                        matches.append(m)
        
        return matches
    
    def match_stereo(self, left_kp: List, left_desc: np.ndarray,
                     right_kp: List, right_desc: np.ndarray,
                     max_y_diff: float = 2.0) -> Tuple[np.ndarray, np.ndarray, List]:
        """
        ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ (ì—í”¼í´ë¼ ì œì•½)
        
        ì •ë¥˜ëœ ì´ë¯¸ì§€ì—ì„œëŠ” ë§¤ì¹­ì ì´ ê°™ì€ í–‰ì— ìˆì–´ì•¼ í•¨
        
        Returns:
        - left_pts: ì™¼ìª½ ì´ë¯¸ì§€ ì ë“¤ [N, 2]
        - right_pts: ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ ì ë“¤ [N, 2]
        - matches: ìœ íš¨í•œ ë§¤ì¹­
        """
        
        matches = self.match(left_desc, right_desc)
        
        valid_matches = []
        left_pts = []
        right_pts = []
        
        for m in matches:
            pt_l = left_kp[m.queryIdx].pt
            pt_r = right_kp[m.trainIdx].pt
            
            # ì—í”¼í´ë¼ ì œì•½: y ì¢Œí‘œ ì°¨ì´ê°€ ì‘ì•„ì•¼ í•¨
            if abs(pt_l[1] - pt_r[1]) < max_y_diff:
                # ì‹œì°¨ ì–‘ìˆ˜ í™•ì¸ (ì™¼ìª½ì´ ì˜¤ë¥¸ìª½ë³´ë‹¤ x ì¢Œí‘œê°€ í¼)
                if pt_l[0] > pt_r[0]:
                    valid_matches.append(m)
                    left_pts.append(pt_l)
                    right_pts.append(pt_r)
        
        return np.array(left_pts), np.array(right_pts), valid_matches
    
    def match_temporal(self, kp_prev: List, desc_prev: np.ndarray,
                       kp_curr: List, desc_curr: np.ndarray,
                       max_distance: float = 100) -> Tuple[np.ndarray, np.ndarray, List]:
        """
        ì‹œê°„ì  ë§¤ì¹­ (í”„ë ˆì„ ê°„)
        
        Returns:
        - prev_pts: ì´ì „ í”„ë ˆì„ ì ë“¤
        - curr_pts: í˜„ì¬ í”„ë ˆì„ ì ë“¤
        - matches: ë§¤ì¹­
        """
        
        matches = self.match(desc_prev, desc_curr)
        
        valid_matches = []
        prev_pts = []
        curr_pts = []
        
        for m in matches:
            pt_prev = kp_prev[m.queryIdx].pt
            pt_curr = kp_curr[m.trainIdx].pt
            
            # ê±°ë¦¬ ì œí•œ
            dist = np.sqrt((pt_prev[0] - pt_curr[0])**2 + 
                          (pt_prev[1] - pt_curr[1])**2)
            
            if dist < max_distance:
                valid_matches.append(m)
                prev_pts.append(pt_prev)
                curr_pts.append(pt_curr)
        
        return np.array(prev_pts), np.array(curr_pts), valid_matches
```

---

## 4. ìŠ¤í…Œë ˆì˜¤ 3D ì¬êµ¬ì„±

### 4.1 ì‚¼ê°ì¸¡ëŸ‰

```python
"""
stereo_triangulation.py
ìŠ¤í…Œë ˆì˜¤ ì‚¼ê°ì¸¡ëŸ‰
"""

import cv2
import numpy as np
from typing import Tuple, Optional
import yaml


class StereoTriangulator:
    """ìŠ¤í…Œë ˆì˜¤ ì‚¼ê°ì¸¡ëŸ‰"""
    
    def __init__(self, calibration_file: str):
        """ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¡œë“œ"""
        
        with open(calibration_file, 'r') as f:
            params = yaml.safe_load(f)
        
        self.P1 = np.array(params['P1'])  # ì™¼ìª½ íˆ¬ì˜ í–‰ë ¬
        self.P2 = np.array(params['P2'])  # ì˜¤ë¥¸ìª½ íˆ¬ì˜ í–‰ë ¬
        self.Q = np.array(params['Q'])    # ì‹œì°¨â†’3D ë³€í™˜ í–‰ë ¬
        
        self.baseline = params['baseline_mm']
        self.fx = self.P1[0, 0]
        self.fy = self.P1[1, 1]
        self.cx = self.P1[0, 2]
        self.cy = self.P1[1, 2]
    
    def triangulate(self, left_pts: np.ndarray, 
                    right_pts: np.ndarray) -> np.ndarray:
        """
        2D ì ë“¤ì„ 3Dë¡œ ì‚¼ê°ì¸¡ëŸ‰
        
        Parameters:
        - left_pts: ì™¼ìª½ ì´ë¯¸ì§€ ì ë“¤ [N, 2]
        - right_pts: ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ ì ë“¤ [N, 2]
        
        Returns:
        - points_3d: 3D ì ë“¤ [N, 3] (mm ë‹¨ìœ„)
        """
        
        if len(left_pts) == 0:
            return np.array([])
        
        # OpenCV triangulatePointsëŠ” [2, N] í˜•íƒœ í•„ìš”
        left_pts_T = left_pts.T.astype(np.float64)
        right_pts_T = right_pts.T.astype(np.float64)
        
        # ë™ì°¨ ì¢Œí‘œë¡œ ì‚¼ê°ì¸¡ëŸ‰
        points_4d = cv2.triangulatePoints(
            self.P1, self.P2,
            left_pts_T, right_pts_T
        )
        
        # ë™ì°¨ ì¢Œí‘œ â†’ ìœ í´ë¦¬ë“œ ì¢Œí‘œ
        points_3d = points_4d[:3] / points_4d[3]
        points_3d = points_3d.T  # [N, 3]
        
        return points_3d
    
    def triangulate_from_disparity(self, left_pts: np.ndarray,
                                    disparities: np.ndarray) -> np.ndarray:
        """
        ì‹œì°¨ë¥¼ ì´ìš©í•œ ì‚¼ê°ì¸¡ëŸ‰
        
        Z = f * B / d
        X = (u - cx) * Z / fx
        Y = (v - cy) * Z / fy
        """
        
        if len(left_pts) == 0:
            return np.array([])
        
        # ìœ íš¨í•œ ì‹œì°¨ë§Œ
        valid = disparities > 0
        
        u = left_pts[valid, 0]
        v = left_pts[valid, 1]
        d = disparities[valid]
        
        Z = self.fx * self.baseline / d
        X = (u - self.cx) * Z / self.fx
        Y = (v - self.cy) * Z / self.fy
        
        points_3d = np.column_stack([X, Y, Z])
        
        return points_3d
    
    def reproject_to_image(self, points_3d: np.ndarray,
                           camera: str = 'left') -> np.ndarray:
        """
        3D ì ì„ ì´ë¯¸ì§€ë¡œ ì¬íˆ¬ì˜
        
        Parameters:
        - points_3d: 3D ì ë“¤ [N, 3]
        - camera: 'left' ë˜ëŠ” 'right'
        
        Returns:
        - points_2d: 2D ì ë“¤ [N, 2]
        """
        
        P = self.P1 if camera == 'left' else self.P2
        
        # ë™ì°¨ ì¢Œí‘œ
        points_h = np.hstack([points_3d, np.ones((len(points_3d), 1))])
        
        # íˆ¬ì˜
        projected = (P @ points_h.T).T
        
        # ì •ê·œí™”
        points_2d = projected[:, :2] / projected[:, 2:3]
        
        return points_2d
```

---

## 5. ëª¨ì…˜ ì¶”ì •

### 5.1 PnP ê¸°ë°˜ ëª¨ì…˜ ì¶”ì •

```python
"""
motion_estimator.py
ì¹´ë©”ë¼ ëª¨ì…˜ ì¶”ì •
"""

import cv2
import numpy as np
from typing import Tuple, Optional
import yaml


class MotionEstimator:
    """ì¹´ë©”ë¼ ëª¨ì…˜ ì¶”ì •"""
    
    def __init__(self, calibration_file: str):
        """ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¡œë“œ"""
        
        with open(calibration_file, 'r') as f:
            params = yaml.safe_load(f)
        
        self.K = np.array(params['K1'])  # ë‚´ë¶€ íŒŒë¼ë¯¸í„°
        self.dist = np.array(params['D1'])  # ì™œê³¡ ê³„ìˆ˜
        
        self.fx = self.K[0, 0]
        self.fy = self.K[1, 1]
        self.cx = self.K[0, 2]
        self.cy = self.K[1, 2]
    
    def estimate_motion_pnp(self, points_3d: np.ndarray,
                            points_2d: np.ndarray,
                            method: str = 'RANSAC') -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        PnPë¡œ ëª¨ì…˜ ì¶”ì •
        
        ì´ì „ í”„ë ˆì„ì˜ 3D ì ë“¤ê³¼ í˜„ì¬ í”„ë ˆì„ì˜ 2D íˆ¬ì˜ì„ ì´ìš©
        
        Parameters:
        - points_3d: ì´ì „ í”„ë ˆì„ 3D ì ë“¤ [N, 3]
        - points_2d: í˜„ì¬ í”„ë ˆì„ 2D ì ë“¤ [N, 2]
        
        Returns:
        - R: íšŒì „ í–‰ë ¬ [3, 3]
        - t: ì´ë™ ë²¡í„° [3, 1]
        - inliers: ì¸ë¼ì´ì–´ ì¸ë±ìŠ¤
        """
        
        if len(points_3d) < 4:
            return None, None, None
        
        # PnP ë°©ë²• ì„ íƒ
        if method == 'RANSAC':
            success, rvec, tvec, inliers = cv2.solvePnPRansac(
                points_3d.astype(np.float64),
                points_2d.astype(np.float64),
                self.K,
                self.dist,
                iterationsCount=1000,
                reprojectionError=2.0,
                confidence=0.99,
                flags=cv2.SOLVEPNP_ITERATIVE
            )
        else:
            success, rvec, tvec = cv2.solvePnP(
                points_3d.astype(np.float64),
                points_2d.astype(np.float64),
                self.K,
                self.dist,
                flags=cv2.SOLVEPNP_ITERATIVE
            )
            inliers = np.arange(len(points_3d))
        
        if not success:
            return None, None, None
        
        # íšŒì „ ë²¡í„° â†’ íšŒì „ í–‰ë ¬
        R, _ = cv2.Rodrigues(rvec)
        t = tvec
        
        return R, t, inliers.flatten() if inliers is not None else None
    
    def estimate_motion_essential(self, pts_prev: np.ndarray,
                                   pts_curr: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Essential Matrixë¡œ ëª¨ì…˜ ì¶”ì •
        
        ëª¨ë…¸í˜ëŸ¬ ë°©ì‹ (ìŠ¤ì¼€ì¼ ëª¨ë¦„)
        
        Returns:
        - R: íšŒì „ í–‰ë ¬
        - t: ì´ë™ ë²¡í„° (ë‹¨ìœ„ ë²¡í„°)
        - mask: ì¸ë¼ì´ì–´ ë§ˆìŠ¤í¬
        """
        
        if len(pts_prev) < 5:
            return None, None, None
        
        # Essential Matrix ì¶”ì •
        E, mask = cv2.findEssentialMat(
            pts_prev, pts_curr,
            self.K,
            method=cv2.RANSAC,
            prob=0.999,
            threshold=1.0
        )
        
        if E is None:
            return None, None, None
        
        # ëª¨ì…˜ ë³µì›
        _, R, t, mask_pose = cv2.recoverPose(
            E, pts_prev, pts_curr, self.K, mask=mask
        )
        
        return R, t, mask.flatten()
    
    def estimate_motion_3d_3d(self, points_3d_prev: np.ndarray,
                              points_3d_curr: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        3D-3D ë§¤ì¹­ìœ¼ë¡œ ëª¨ì…˜ ì¶”ì • (ICP ìŠ¤íƒ€ì¼)
        
        Returns:
        - R: íšŒì „ í–‰ë ¬
        - t: ì´ë™ ë²¡í„°
        """
        
        if len(points_3d_prev) < 3:
            return None, None
        
        # ì¤‘ì‹¬ì 
        centroid_prev = np.mean(points_3d_prev, axis=0)
        centroid_curr = np.mean(points_3d_curr, axis=0)
        
        # ì¤‘ì‹¬ ì´ë™
        prev_centered = points_3d_prev - centroid_prev
        curr_centered = points_3d_curr - centroid_curr
        
        # SVDë¡œ íšŒì „ ê³„ì‚°
        H = prev_centered.T @ curr_centered
        U, S, Vt = np.linalg.svd(H)
        R = Vt.T @ U.T
        
        # ë°˜ì‚¬ ì²´í¬
        if np.linalg.det(R) < 0:
            Vt[-1, :] *= -1
            R = Vt.T @ U.T
        
        # ì´ë™ ê³„ì‚°
        t = centroid_curr - R @ centroid_prev
        
        return R, t.reshape(3, 1)
    
    def compose_transformation(self, R: np.ndarray, 
                               t: np.ndarray) -> np.ndarray:
        """
        íšŒì „/ì´ë™ì„ 4x4 ë³€í™˜ í–‰ë ¬ë¡œ ê²°í•©
        
        Returns:
        - T: [4, 4] ë³€í™˜ í–‰ë ¬
        """
        
        T = np.eye(4)
        T[:3, :3] = R
        T[:3, 3] = t.flatten()
        
        return T
    
    def decompose_transformation(self, T: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        4x4 ë³€í™˜ í–‰ë ¬ì„ íšŒì „/ì´ë™ìœ¼ë¡œ ë¶„í•´
        """
        
        R = T[:3, :3]
        t = T[:3, 3].reshape(3, 1)
        
        return R, t
```

### 5.2 RANSAC ê¸°ë°˜ ê°•ê±´ ì¶”ì •

```python
"""
robust_estimator.py
ê°•ê±´í•œ ëª¨ì…˜ ì¶”ì •
"""

import numpy as np
from typing import Tuple, Optional, Callable


class RANSACEstimator:
    """RANSAC ê¸°ë°˜ ê°•ê±´ ì¶”ì •"""
    
    def __init__(self, min_samples: int = 4,
                 residual_threshold: float = 2.0,
                 max_trials: int = 1000,
                 confidence: float = 0.99):
        """
        Parameters:
        - min_samples: ëª¨ë¸ ì¶”ì •ì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        - residual_threshold: ì¸ë¼ì´ì–´ íŒì • ì„ê³„ê°’
        - max_trials: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
        - confidence: ì‹ ë¢°ë„
        """
        
        self.min_samples = min_samples
        self.residual_threshold = residual_threshold
        self.max_trials = max_trials
        self.confidence = confidence
    
    def estimate(self, data: np.ndarray,
                 model_func: Callable,
                 residual_func: Callable) -> Tuple[any, np.ndarray]:
        """
        RANSAC ì¶”ì •
        
        Parameters:
        - data: ë°ì´í„° [N, ...]
        - model_func: ëª¨ë¸ ì¶”ì • í•¨ìˆ˜ (samples -> model)
        - residual_func: ì”ì°¨ ê³„ì‚° í•¨ìˆ˜ (model, data -> residuals)
        
        Returns:
        - best_model: ìµœì  ëª¨ë¸
        - inliers: ì¸ë¼ì´ì–´ ë§ˆìŠ¤í¬
        """
        
        n_samples = len(data)
        best_model = None
        best_inliers = np.zeros(n_samples, dtype=bool)
        best_n_inliers = 0
        
        # ë™ì  ë°˜ë³µ íšŸìˆ˜ ê³„ì‚°
        n_trials = self.max_trials
        trial = 0
        
        while trial < n_trials:
            # ëœë¤ ìƒ˜í”Œ ì„ íƒ
            indices = np.random.choice(n_samples, self.min_samples, replace=False)
            samples = data[indices]
            
            # ëª¨ë¸ ì¶”ì •
            try:
                model = model_func(samples)
            except:
                trial += 1
                continue
            
            if model is None:
                trial += 1
                continue
            
            # ì”ì°¨ ê³„ì‚°
            residuals = residual_func(model, data)
            
            # ì¸ë¼ì´ì–´ íŒì •
            inliers = np.abs(residuals) < self.residual_threshold
            n_inliers = np.sum(inliers)
            
            # ìµœì  ëª¨ë¸ ì—…ë°ì´íŠ¸
            if n_inliers > best_n_inliers:
                best_n_inliers = n_inliers
                best_model = model
                best_inliers = inliers
                
                # ë°˜ë³µ íšŸìˆ˜ ì¬ê³„ì‚°
                inlier_ratio = n_inliers / n_samples
                if inlier_ratio > 0:
                    n_trials = int(np.log(1 - self.confidence) / 
                                  np.log(1 - inlier_ratio ** self.min_samples))
                    n_trials = min(n_trials, self.max_trials)
            
            trial += 1
        
        # ì¸ë¼ì´ì–´ë¡œ ëª¨ë¸ ì¬ì¶”ì • (ì„ íƒ)
        if best_n_inliers >= self.min_samples:
            try:
                best_model = model_func(data[best_inliers])
            except:
                pass
        
        return best_model, best_inliers
```

---

## 6. ê¶¤ì  ì¶”ì 

### 6.1 í¬ì¦ˆ ëˆ„ì  ë° ê¶¤ì 

```python
"""
trajectory_tracker.py
ì¹´ë©”ë¼ ê¶¤ì  ì¶”ì 
"""

import numpy as np
from typing import List, Tuple, Optional
from dataclasses import dataclass, field
import json


@dataclass
class Pose:
    """ì¹´ë©”ë¼ í¬ì¦ˆ"""
    timestamp: float
    position: np.ndarray      # [x, y, z]
    rotation: np.ndarray      # [3, 3] íšŒì „ í–‰ë ¬
    transformation: np.ndarray = None  # [4, 4] ì „ì²´ ë³€í™˜
    
    def __post_init__(self):
        if self.transformation is None:
            self.transformation = np.eye(4)
            self.transformation[:3, :3] = self.rotation
            self.transformation[:3, 3] = self.position


class TrajectoryTracker:
    """ê¶¤ì  ì¶”ì ê¸°"""
    
    def __init__(self):
        self.poses: List[Pose] = []
        
        # í˜„ì¬ ê¸€ë¡œë²Œ í¬ì¦ˆ
        self.current_pose = np.eye(4)
        
        # í”„ë ˆì„ ì¹´ìš´í„°
        self.frame_count = 0
    
    def update(self, R: np.ndarray, t: np.ndarray, 
               timestamp: float = None) -> Pose:
        """
        ìƒˆë¡œìš´ ìƒëŒ€ ëª¨ì…˜ìœ¼ë¡œ í¬ì¦ˆ ì—…ë°ì´íŠ¸
        
        Parameters:
        - R: ìƒëŒ€ íšŒì „ [3, 3]
        - t: ìƒëŒ€ ì´ë™ [3, 1]
        - timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        
        Returns:
        - pose: ìƒˆë¡œìš´ ê¸€ë¡œë²Œ í¬ì¦ˆ
        """
        
        if timestamp is None:
            timestamp = self.frame_count
        
        # ìƒëŒ€ ë³€í™˜ í–‰ë ¬
        T_rel = np.eye(4)
        T_rel[:3, :3] = R
        T_rel[:3, 3] = t.flatten()
        
        # ê¸€ë¡œë²Œ í¬ì¦ˆ ëˆ„ì 
        # T_global = T_global * T_rel
        self.current_pose = self.current_pose @ T_rel
        
        # í¬ì¦ˆ ì €ì¥
        pose = Pose(
            timestamp=timestamp,
            position=self.current_pose[:3, 3].copy(),
            rotation=self.current_pose[:3, :3].copy(),
            transformation=self.current_pose.copy()
        )
        
        self.poses.append(pose)
        self.frame_count += 1
        
        return pose
    
    def get_trajectory(self) -> np.ndarray:
        """
        ì „ì²´ ê¶¤ì  ë°˜í™˜
        
        Returns:
        - trajectory: [N, 3] ìœ„ì¹˜ ë°°ì—´
        """
        
        if len(self.poses) == 0:
            return np.array([])
        
        return np.array([p.position for p in self.poses])
    
    def get_poses(self) -> List[np.ndarray]:
        """ëª¨ë“  í¬ì¦ˆ ë³€í™˜ í–‰ë ¬ ë°˜í™˜"""
        return [p.transformation for p in self.poses]
    
    def get_current_position(self) -> np.ndarray:
        """í˜„ì¬ ìœ„ì¹˜"""
        return self.current_pose[:3, 3]
    
    def get_current_rotation(self) -> np.ndarray:
        """í˜„ì¬ íšŒì „"""
        return self.current_pose[:3, :3]
    
    def get_total_distance(self) -> float:
        """ì´ ì´ë™ ê±°ë¦¬"""
        
        trajectory = self.get_trajectory()
        
        if len(trajectory) < 2:
            return 0.0
        
        distances = np.linalg.norm(np.diff(trajectory, axis=0), axis=1)
        
        return np.sum(distances)
    
    def reset(self):
        """ë¦¬ì…‹"""
        self.poses.clear()
        self.current_pose = np.eye(4)
        self.frame_count = 0
    
    def save(self, filename: str):
        """ê¶¤ì  ì €ì¥"""
        
        data = {
            'num_poses': len(self.poses),
            'trajectory': self.get_trajectory().tolist(),
            'poses': [
                {
                    'timestamp': p.timestamp,
                    'position': p.position.tolist(),
                    'rotation': p.rotation.tolist()
                }
                for p in self.poses
            ]
        }
        
        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"âœ… ê¶¤ì  ì €ì¥: {filename}")
    
    def load(self, filename: str):
        """ê¶¤ì  ë¡œë“œ"""
        
        with open(filename, 'r') as f:
            data = json.load(f)
        
        self.reset()
        
        for pose_data in data['poses']:
            pose = Pose(
                timestamp=pose_data['timestamp'],
                position=np.array(pose_data['position']),
                rotation=np.array(pose_data['rotation'])
            )
            self.poses.append(pose)
        
        if len(self.poses) > 0:
            self.current_pose = self.poses[-1].transformation
            self.frame_count = len(self.poses)
        
        print(f"âœ… ê¶¤ì  ë¡œë“œ: {len(self.poses)} poses")
```

---

## 7. ìµœì í™” ë° ë“œë¦¬í”„íŠ¸ ë³´ì •

### 7.1 ë¡œì»¬ ë²ˆë“¤ ì¡°ì •

```python
"""
bundle_adjustment.py
ë²ˆë“¤ ì¡°ì • (ìµœì í™”)
"""

import numpy as np
from scipy.optimize import least_squares
from typing import List, Tuple, Optional


class LocalBundleAdjustment:
    """ë¡œì»¬ ë²ˆë“¤ ì¡°ì •"""
    
    def __init__(self, window_size: int = 5):
        """
        Parameters:
        - window_size: ìµœì í™”í•  ìµœê·¼ í”„ë ˆì„ ìˆ˜
        """
        
        self.window_size = window_size
    
    def optimize(self, poses: List[np.ndarray],
                 points_3d: List[np.ndarray],
                 observations: List[List[np.ndarray]],
                 K: np.ndarray) -> Tuple[List[np.ndarray], np.ndarray]:
        """
        ë¡œì»¬ ë²ˆë“¤ ì¡°ì •
        
        Parameters:
        - poses: ì¹´ë©”ë¼ í¬ì¦ˆ ë¦¬ìŠ¤íŠ¸ [4x4]
        - points_3d: 3D ì ë“¤
        - observations: ê° í”„ë ˆì„ì—ì„œì˜ 2D ê´€ì¸¡ [[frame_idx, pt_idx, u, v], ...]
        - K: ë‚´ë¶€ íŒŒë¼ë¯¸í„°
        
        Returns:
        - optimized_poses: ìµœì í™”ëœ í¬ì¦ˆ
        - optimized_points: ìµœì í™”ëœ 3D ì 
        """
        
        # ê°„ì†Œí™”ëœ êµ¬í˜„
        # ì‹¤ì œë¡œëŠ” g2o, GTSAM ë“± ì‚¬ìš© ê¶Œì¥
        
        n_poses = len(poses)
        n_points = len(points_3d)
        
        if n_poses < 2 or n_points < 4:
            return poses, np.array(points_3d)
        
        # íŒŒë¼ë¯¸í„° ë²¡í„° êµ¬ì„±
        # [pose_0, pose_1, ..., point_0, point_1, ...]
        
        def residual_func(params):
            """ì”ì°¨ ê³„ì‚°"""
            residuals = []
            
            # íŒŒë¼ë¯¸í„° ë¶„í•´
            # ... (ìƒëµ)
            
            return np.array(residuals)
        
        # ìµœì í™”
        # ... (ìƒëµ)
        
        return poses, np.array(points_3d)


class LoopClosureDetector:
    """ë£¨í”„ í´ë¡œì € ê²€ì¶œ"""
    
    def __init__(self, min_interval: int = 50,
                 similarity_threshold: float = 0.8):
        """
        Parameters:
        - min_interval: ë£¨í”„ ê²€ì¶œ ìµœì†Œ í”„ë ˆì„ ê°„ê²©
        - similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        
        self.min_interval = min_interval
        self.similarity_threshold = similarity_threshold
        
        # í‚¤í”„ë ˆì„ ë””ìŠ¤í¬ë¦½í„°
        self.keyframe_descriptors = []
        self.keyframe_indices = []
    
    def add_keyframe(self, frame_idx: int, descriptors: np.ndarray):
        """í‚¤í”„ë ˆì„ ì¶”ê°€"""
        
        # BoW (Bag of Words) ë²¡í„° ê³„ì‚°
        # ê°„ë‹¨íˆ í‰ê·  ë””ìŠ¤í¬ë¦½í„° ì‚¬ìš©
        if descriptors is not None and len(descriptors) > 0:
            bow = np.mean(descriptors.astype(np.float32), axis=0)
            self.keyframe_descriptors.append(bow)
            self.keyframe_indices.append(frame_idx)
    
    def detect_loop(self, frame_idx: int, 
                    descriptors: np.ndarray) -> Optional[int]:
        """
        ë£¨í”„ í´ë¡œì € ê²€ì¶œ
        
        Returns:
        - matched_idx: ë§¤ì¹­ëœ í‚¤í”„ë ˆì„ ì¸ë±ìŠ¤ ë˜ëŠ” None
        """
        
        if len(self.keyframe_descriptors) == 0:
            return None
        
        if descriptors is None or len(descriptors) == 0:
            return None
        
        # í˜„ì¬ BoW
        current_bow = np.mean(descriptors.astype(np.float32), axis=0)
        
        best_similarity = 0
        best_idx = None
        
        for i, (kf_bow, kf_idx) in enumerate(
            zip(self.keyframe_descriptors, self.keyframe_indices)
        ):
            # ìµœì†Œ ê°„ê²© ì²´í¬
            if frame_idx - kf_idx < self.min_interval:
                continue
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = np.dot(current_bow, kf_bow) / (
                np.linalg.norm(current_bow) * np.linalg.norm(kf_bow) + 1e-6
            )
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_idx = kf_idx
        
        if best_similarity > self.similarity_threshold:
            print(f"ğŸ”„ ë£¨í”„ í´ë¡œì € ê²€ì¶œ: frame {frame_idx} â†’ {best_idx}")
            return best_idx
        
        return None
```

---

## 8. ì‹œê°í™”

### 8.1 ê¶¤ì  ì‹œê°í™”

```python
"""
visualizer.py
Visual Odometry ì‹œê°í™”
"""

import cv2
import numpy as np
from typing import List, Optional, Tuple
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


class VOVisualizer:
    """Visual Odometry ì‹œê°í™”"""
    
    def __init__(self, window_name: str = "Visual Odometry"):
        self.window_name = window_name
        
        # 2D ê¶¤ì  ìº”ë²„ìŠ¤
        self.trajectory_canvas = None
        self.canvas_size = (800, 800)
        self.scale = 1.0  # mm â†’ pixel
        self.offset = np.array([400, 700])  # ìº”ë²„ìŠ¤ ì¤‘ì‹¬
        
        # 3D ê¶¤ì  (matplotlib)
        self.fig_3d = None
        self.ax_3d = None
        
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
    
    def initialize_canvas(self):
        """2D ìº”ë²„ìŠ¤ ì´ˆê¸°í™”"""
        self.trajectory_canvas = np.zeros(
            (self.canvas_size[1], self.canvas_size[0], 3), dtype=np.uint8
        )
        
        # ê·¸ë¦¬ë“œ ê·¸ë¦¬ê¸°
        for i in range(0, self.canvas_size[0], 100):
            cv2.line(self.trajectory_canvas, (i, 0), (i, self.canvas_size[1]),
                    (30, 30, 30), 1)
        for i in range(0, self.canvas_size[1], 100):
            cv2.line(self.trajectory_canvas, (0, i), (self.canvas_size[0], i),
                    (30, 30, 30), 1)
        
        # ì›ì 
        cv2.circle(self.trajectory_canvas, 
                  tuple(self.offset.astype(int)), 5, (0, 0, 255), -1)
    
    def world_to_canvas(self, position: np.ndarray) -> Tuple[int, int]:
        """ì›”ë“œ ì¢Œí‘œ â†’ ìº”ë²„ìŠ¤ ì¢Œí‘œ"""
        
        # X-Z í‰ë©´ íˆ¬ì˜ (YëŠ” ìœ„ìª½)
        x = position[0] * self.scale + self.offset[0]
        y = -position[2] * self.scale + self.offset[1]  # Zê°€ ì•ìª½
        
        return int(x), int(y)
    
    def draw_trajectory_2d(self, trajectory: np.ndarray,
                           current_pose: np.ndarray = None) -> np.ndarray:
        """2D ê¶¤ì  ê·¸ë¦¬ê¸° (íƒ‘ë·°)"""
        
        if self.trajectory_canvas is None:
            self.initialize_canvas()
        
        canvas = self.trajectory_canvas.copy()
        
        if len(trajectory) < 2:
            return canvas
        
        # ìŠ¤ì¼€ì¼ ìë™ ì¡°ì •
        max_range = np.max(np.abs(trajectory[:, [0, 2]]))
        if max_range > 0:
            self.scale = min(300 / max_range, 2.0)
        
        # ê¶¤ì  ê·¸ë¦¬ê¸°
        for i in range(1, len(trajectory)):
            pt1 = self.world_to_canvas(trajectory[i-1])
            pt2 = self.world_to_canvas(trajectory[i])
            
            cv2.line(canvas, pt1, pt2, (0, 255, 0), 2)
        
        # í˜„ì¬ ìœ„ì¹˜
        if len(trajectory) > 0:
            current_pt = self.world_to_canvas(trajectory[-1])
            cv2.circle(canvas, current_pt, 8, (0, 255, 255), -1)
            
            # ë°©í–¥ í‘œì‹œ
            if current_pose is not None:
                direction = current_pose[:3, :3] @ np.array([0, 0, 50])
                end_pt = self.world_to_canvas(trajectory[-1] + direction)
                cv2.arrowedLine(canvas, current_pt, end_pt, (0, 255, 255), 2)
        
        # ì •ë³´ í‘œì‹œ
        total_dist = np.sum(np.linalg.norm(np.diff(trajectory, axis=0), axis=1))
        cv2.putText(canvas, f"Distance: {total_dist/1000:.2f}m", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(canvas, f"Frames: {len(trajectory)}", (10, 60),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        if len(trajectory) > 0:
            pos = trajectory[-1]
            cv2.putText(canvas, f"X: {pos[0]/1000:.2f}m", (10, 90),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            cv2.putText(canvas, f"Y: {pos[1]/1000:.2f}m", (10, 115),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            cv2.putText(canvas, f"Z: {pos[2]/1000:.2f}m", (10, 140),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        return canvas
    
    def draw_feature_matches(self, image: np.ndarray,
                             prev_pts: np.ndarray,
                             curr_pts: np.ndarray,
                             inliers: np.ndarray = None) -> np.ndarray:
        """íŠ¹ì§•ì  ë§¤ì¹­ ì‹œê°í™”"""
        
        display = image.copy()
        
        if prev_pts is None or curr_pts is None:
            return display
        
        for i in range(len(prev_pts)):
            pt1 = tuple(prev_pts[i].astype(int))
            pt2 = tuple(curr_pts[i].astype(int))
            
            color = (0, 255, 0) if (inliers is None or inliers[i]) else (0, 0, 255)
            
            cv2.circle(display, pt2, 3, color, -1)
            cv2.line(display, pt1, pt2, color, 1)
        
        return display
    
    def draw_combined(self, image: np.ndarray,
                      trajectory: np.ndarray,
                      current_pose: np.ndarray,
                      prev_pts: np.ndarray = None,
                      curr_pts: np.ndarray = None) -> np.ndarray:
        """í†µí•© ì‹œê°í™”"""
        
        h, w = image.shape[:2]
        
        # íŠ¹ì§•ì  ë§¤ì¹­ ê·¸ë¦¬ê¸°
        image_display = self.draw_feature_matches(image, prev_pts, curr_pts)
        
        # 2D ê¶¤ì 
        traj_display = self.draw_trajectory_2d(trajectory, current_pose)
        traj_display = cv2.resize(traj_display, (w // 2, h // 2))
        
        # ê²°í•©
        combined = np.zeros((h, w + w // 2, 3), dtype=np.uint8)
        combined[:, :w] = image_display
        combined[:h//2, w:] = traj_display
        
        return combined
    
    def show(self, image: np.ndarray):
        """ì´ë¯¸ì§€ í‘œì‹œ"""
        cv2.imshow(self.window_name, image)
    
    def wait_key(self, delay: int = 1) -> int:
        """í‚¤ ì…ë ¥"""
        return cv2.waitKey(delay) & 0xFF
    
    def plot_3d_trajectory(self, trajectory: np.ndarray,
                           show: bool = True):
        """3D ê¶¤ì  í”Œë¡¯ (matplotlib)"""
        
        if len(trajectory) == 0:
            return
        
        if self.fig_3d is None:
            self.fig_3d = plt.figure(figsize=(10, 8))
            self.ax_3d = self.fig_3d.add_subplot(111, projection='3d')
        
        self.ax_3d.clear()
        
        # ê¶¤ì 
        self.ax_3d.plot(trajectory[:, 0], trajectory[:, 2], trajectory[:, 1],
                       'b-', linewidth=2, label='Trajectory')
        
        # ì‹œì‘ì 
        self.ax_3d.scatter(trajectory[0, 0], trajectory[0, 2], trajectory[0, 1],
                          c='g', s=100, marker='o', label='Start')
        
        # ëì 
        self.ax_3d.scatter(trajectory[-1, 0], trajectory[-1, 2], trajectory[-1, 1],
                          c='r', s=100, marker='x', label='End')
        
        self.ax_3d.set_xlabel('X (mm)')
        self.ax_3d.set_ylabel('Z (mm)')
        self.ax_3d.set_zlabel('Y (mm)')
        self.ax_3d.legend()
        self.ax_3d.set_title('Camera Trajectory')
        
        # ì¶• ë¹„ìœ¨ ë™ì¼í•˜ê²Œ
        max_range = np.max(np.abs(trajectory))
        self.ax_3d.set_xlim([-max_range, max_range])
        self.ax_3d.set_ylim([0, max_range * 2])
        self.ax_3d.set_zlim([-max_range, max_range])
        
        if show:
            plt.show()
        else:
            plt.savefig('trajectory_3d.png')
    
    def close(self):
        """ì¢…ë£Œ"""
        cv2.destroyAllWindows()
        if self.fig_3d:
            plt.close(self.fig_3d)
```

---

## 9. ì „ì²´ ì½”ë“œ

### 9.1 Visual Odometry ë©”ì¸ í´ë˜ìŠ¤

```python
"""
visual_odometry.py
Stereo Visual Odometry ë©”ì¸ í´ë˜ìŠ¤
"""

import cv2
import numpy as np
from typing import Optional, Tuple
import yaml
import time

from feature_detector import FeatureDetector, FeatureTracker, DetectorType
from feature_matcher import FeatureMatcher
from stereo_triangulation import StereoTriangulator
from motion_estimator import MotionEstimator
from trajectory_tracker import TrajectoryTracker


class StereoVisualOdometry:
    """Stereo Visual Odometry"""
    
    def __init__(self, calibration_file: str, config: dict = None):
        """
        Parameters:
        - calibration_file: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒì¼
        - config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        
        self.config = config or {}
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.detector = FeatureDetector(
            detector_type=DetectorType.ORB,
            max_features=self.config.get('max_features', 2000)
        )
        
        self.tracker = FeatureTracker(
            max_features=self.config.get('max_features', 2000)
        )
        
        self.matcher = FeatureMatcher(
            matcher_type='BF',
            cross_check=False,
            ratio_threshold=0.75
        )
        
        self.triangulator = StereoTriangulator(calibration_file)
        self.motion_estimator = MotionEstimator(calibration_file)
        self.trajectory = TrajectoryTracker()
        
        # ì´ì „ í”„ë ˆì„ ë°ì´í„°
        self.prev_left = None
        self.prev_kp = None
        self.prev_desc = None
        self.prev_points_3d = None
        
        # ìƒíƒœ
        self.frame_count = 0
        self.is_initialized = False
        
        # í†µê³„
        self.processing_times = []
    
    def process_frame(self, left: np.ndarray, 
                      right: np.ndarray) -> Tuple[Optional[np.ndarray], dict]:
        """
        í”„ë ˆì„ ì²˜ë¦¬
        
        Parameters:
        - left: ì •ë¥˜ëœ ì™¼ìª½ ì´ë¯¸ì§€
        - right: ì •ë¥˜ëœ ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€
        
        Returns:
        - pose: í˜„ì¬ í¬ì¦ˆ [4, 4] ë˜ëŠ” None
        - info: ì²˜ë¦¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        
        start_time = time.time()
        
        info = {
            'frame_id': self.frame_count,
            'num_features': 0,
            'num_matches': 0,
            'num_inliers': 0,
            'tracking_quality': 'unknown'
        }
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
        gray_left = cv2.cvtColor(left, cv2.COLOR_BGR2GRAY) if len(left.shape) == 3 else left
        gray_right = cv2.cvtColor(right, cv2.COLOR_BGR2GRAY) if len(right.shape) == 3 else right
        
        # 1. íŠ¹ì§•ì  ê²€ì¶œ
        curr_kp, curr_desc = self.detector.detect(gray_left)
        info['num_features'] = len(curr_kp)
        
        if len(curr_kp) < 10:
            info['tracking_quality'] = 'poor'
            return None, info
        
        # 2. ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ â†’ 3D ì 
        stereo_data = self.detector.detect_and_compute_stereo(gray_left, gray_right)
        
        left_pts, right_pts, stereo_matches = self.matcher.match_stereo(
            stereo_data['left_kp'], stereo_data['left_desc'],
            stereo_data['right_kp'], stereo_data['right_desc']
        )
        
        if len(left_pts) < 10:
            info['tracking_quality'] = 'poor'
            return None, info
        
        # 3D ì‚¼ê°ì¸¡ëŸ‰
        curr_points_3d = self.triangulator.triangulate(left_pts, right_pts)
        
        # ì²« í”„ë ˆì„
        if not self.is_initialized:
            self.prev_left = gray_left
            self.prev_kp = curr_kp
            self.prev_desc = curr_desc
            self.prev_points_3d = curr_points_3d
            self.prev_left_pts = left_pts
            self.is_initialized = True
            
            # ì´ˆê¸° í¬ì¦ˆ ì €ì¥
            self.trajectory.update(np.eye(3), np.zeros((3, 1)), self.frame_count)
            
            self.frame_count += 1
            info['tracking_quality'] = 'initialized'
            
            return self.trajectory.current_pose, info
        
        # 3. ì‹œê°„ì  ë§¤ì¹­
        prev_matched, curr_matched, temporal_matches = self.matcher.match_temporal(
            self.prev_kp, self.prev_desc,
            curr_kp, curr_desc
        )
        
        info['num_matches'] = len(temporal_matches)
        
        if len(temporal_matches) < 10:
            info['tracking_quality'] = 'lost'
            # ì¬ì´ˆê¸°í™”
            self.prev_left = gray_left
            self.prev_kp = curr_kp
            self.prev_desc = curr_desc
            self.prev_points_3d = curr_points_3d
            return None, info
        
        # 4. ë§¤ì¹­ëœ 3D ì  ì°¾ê¸°
        # ì´ì „ í”„ë ˆì„ì˜ ë§¤ì¹­ëœ í‚¤í¬ì¸íŠ¸ â†’ 3D ì  ì°¾ê¸°
        matched_3d = []
        matched_2d = []
        
        for match in temporal_matches:
            prev_pt = self.prev_kp[match.queryIdx].pt
            
            # ì´ì „ í”„ë ˆì„ì˜ ìŠ¤í…Œë ˆì˜¤ ë§¤ì¹­ëœ ì ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì  ì°¾ê¸°
            min_dist = float('inf')
            closest_3d = None
            
            for i, pt_2d in enumerate(self.prev_left_pts):
                dist = np.linalg.norm(np.array(prev_pt) - pt_2d)
                if dist < min_dist and dist < 5:  # 5í”½ì…€ ì´ë‚´
                    min_dist = dist
                    closest_3d = self.prev_points_3d[i]
            
            if closest_3d is not None:
                matched_3d.append(closest_3d)
                matched_2d.append(curr_kp[match.trainIdx].pt)
        
        matched_3d = np.array(matched_3d)
        matched_2d = np.array(matched_2d)
        
        if len(matched_3d) < 6:
            info['tracking_quality'] = 'insufficient'
            return None, info
        
        # 5. PnPë¡œ ëª¨ì…˜ ì¶”ì •
        R, t, inliers = self.motion_estimator.estimate_motion_pnp(
            matched_3d, matched_2d
        )
        
        if R is None:
            info['tracking_quality'] = 'failed'
            return None, info
        
        info['num_inliers'] = len(inliers) if inliers is not None else 0
        
        # ì¸ë¼ì´ì–´ ë¹„ìœ¨ë¡œ í’ˆì§ˆ íŒë‹¨
        inlier_ratio = info['num_inliers'] / len(matched_3d)
        if inlier_ratio > 0.7:
            info['tracking_quality'] = 'good'
        elif inlier_ratio > 0.5:
            info['tracking_quality'] = 'fair'
        else:
            info['tracking_quality'] = 'poor'
        
        # 6. í¬ì¦ˆ ì—…ë°ì´íŠ¸
        # PnP ê²°ê³¼ëŠ” ì¹´ë©”ë¼â†’ì›”ë“œ ë³€í™˜ì´ë¯€ë¡œ ì—­ë³€í™˜ í•„ìš”
        R_inv = R.T
        t_inv = -R.T @ t
        
        pose = self.trajectory.update(R_inv, t_inv, self.frame_count)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.prev_left = gray_left
        self.prev_kp = curr_kp
        self.prev_desc = curr_desc
        self.prev_points_3d = curr_points_3d
        self.prev_left_pts = left_pts
        
        self.frame_count += 1
        
        # ì²˜ë¦¬ ì‹œê°„
        elapsed = time.time() - start_time
        self.processing_times.append(elapsed)
        info['processing_time_ms'] = elapsed * 1000
        
        return self.trajectory.current_pose, info
    
    def get_trajectory(self) -> np.ndarray:
        """ê¶¤ì  ë°˜í™˜"""
        return self.trajectory.get_trajectory()
    
    def get_current_pose(self) -> np.ndarray:
        """í˜„ì¬ í¬ì¦ˆ ë°˜í™˜"""
        return self.trajectory.current_pose
    
    def get_statistics(self) -> dict:
        """í†µê³„ ë°˜í™˜"""
        return {
            'total_frames': self.frame_count,
            'total_distance': self.trajectory.get_total_distance(),
            'avg_processing_time_ms': np.mean(self.processing_times) * 1000 if self.processing_times else 0
        }
    
    def reset(self):
        """ë¦¬ì…‹"""
        self.prev_left = None
        self.prev_kp = None
        self.prev_desc = None
        self.prev_points_3d = None
        self.frame_count = 0
        self.is_initialized = False
        self.trajectory.reset()
        self.processing_times.clear()
```

### 9.2 ë©”ì¸ ì‹¤í–‰ íŒŒì¼

```python
"""
main.py
Visual Odometry ë©”ì¸
"""

import argparse
import yaml
import cv2
import numpy as np
import sys

from stereo_camera import StereoCamera
from visual_odometry import StereoVisualOdometry
from visualizer import VOVisualizer


def main():
    parser = argparse.ArgumentParser(description='Stereo Visual Odometry')
    parser.add_argument('--config', default='config/vo_config.yaml')
    parser.add_argument('--save', action='store_true', help='ê¶¤ì  ì €ì¥')
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    print("\n" + "="*60)
    print("ğŸ“ Stereo Visual Odometry")
    print("="*60)
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    camera = StereoCamera(
        config['calibration_file'],
        config['camera']['left_id'],
        config['camera']['right_id']
    )
    
    vo = StereoVisualOdometry(
        config['calibration_file'],
        config.get('vo', {})
    )
    
    visualizer = VOVisualizer()
    
    print("\nì¡°ì‘ ë°©ë²•:")
    print("  [R] - ë¦¬ì…‹")
    print("  [S] - ê¶¤ì  ì €ì¥")
    print("  [3] - 3D ë·°")
    print("  [Q] - ì¢…ë£Œ")
    print("="*60 + "\n")
    
    prev_pts = None
    curr_pts = None
    
    while True:
        # í”„ë ˆì„ ìº¡ì²˜
        rect_left, rect_right = camera.capture_rectified()
        
        if rect_left is None:
            continue
        
        # VO ì²˜ë¦¬
        pose, info = vo.process_frame(rect_left, rect_right)
        
        # ê¶¤ì 
        trajectory = vo.get_trajectory()
        
        # ì‹œê°í™”
        display = visualizer.draw_combined(
            rect_left, trajectory, pose, prev_pts, curr_pts
        )
        
        # ì •ë³´ í‘œì‹œ
        quality = info.get('tracking_quality', 'unknown')
        color = (0, 255, 0) if quality == 'good' else (0, 255, 255) if quality == 'fair' else (0, 0, 255)
        
        cv2.putText(display, f"Quality: {quality}", (10, display.shape[0] - 60),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        cv2.putText(display, f"Features: {info.get('num_features', 0)}", (10, display.shape[0] - 35),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        cv2.putText(display, f"Inliers: {info.get('num_inliers', 0)}", (10, display.shape[0] - 15),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        visualizer.show(display)
        
        # í‚¤ ì²˜ë¦¬
        key = visualizer.wait_key(1)
        
        if key == ord('q'):
            break
        elif key == ord('r'):
            vo.reset()
            visualizer.initialize_canvas()
            print("ğŸ”„ ë¦¬ì…‹")
        elif key == ord('s'):
            vo.trajectory.save('output/trajectory.json')
        elif key == ord('3'):
            visualizer.plot_3d_trajectory(trajectory)
    
    # ì¢…ë£Œ
    stats = vo.get_statistics()
    print("\n" + "="*60)
    print("ğŸ“Š ê²°ê³¼ ìš”ì•½")
    print(f"  ì´ í”„ë ˆì„: {stats['total_frames']}")
    print(f"  ì´ ì´ë™ê±°ë¦¬: {stats['total_distance']/1000:.2f} m")
    print(f"  í‰ê·  ì²˜ë¦¬ì‹œê°„: {stats['avg_processing_time_ms']:.1f} ms")
    print("="*60)
    
    if args.save:
        vo.trajectory.save('output/trajectory.json')
    
    camera.release()
    visualizer.close()


if __name__ == "__main__":
    main()
```

### 9.3 ì„¤ì • íŒŒì¼

```yaml
# config/vo_config.yaml

calibration_file: "config/stereo_params.yaml"

camera:
  left_id: 0
  right_id: 2
  width: 1280
  height: 720

vo:
  max_features: 2000
  detector: "ORB"         # ORB, SIFT, FAST
  matcher: "BF"           # BF, FLANN
  min_inliers: 10
  ransac_threshold: 2.0

output:
  save_trajectory: true
  output_dir: "output/trajectories"
```

---

## 10. ì„±ëŠ¥ í‰ê°€

### 10.1 í‰ê°€ ë©”íŠ¸ë¦­

```python
"""
evaluation.py
VO ì„±ëŠ¥ í‰ê°€
"""

import numpy as np
from typing import List, Tuple


def compute_ate(estimated: np.ndarray, 
                ground_truth: np.ndarray) -> Tuple[float, float]:
    """
    ATE (Absolute Trajectory Error) ê³„ì‚°
    
    Returns:
    - rmse: RMSE
    - mean: í‰ê·  ì˜¤ì°¨
    """
    
    errors = np.linalg.norm(estimated - ground_truth, axis=1)
    
    rmse = np.sqrt(np.mean(errors ** 2))
    mean_error = np.mean(errors)
    
    return rmse, mean_error


def compute_rpe(estimated: np.ndarray,
                ground_truth: np.ndarray,
                delta: int = 1) -> Tuple[float, float]:
    """
    RPE (Relative Pose Error) ê³„ì‚°
    
    Parameters:
    - delta: í”„ë ˆì„ ê°„ê²©
    
    Returns:
    - trans_error: ì´ë™ ì˜¤ì°¨
    - rot_error: íšŒì „ ì˜¤ì°¨ (ë„)
    """
    
    trans_errors = []
    
    for i in range(len(estimated) - delta):
        est_rel = estimated[i + delta] - estimated[i]
        gt_rel = ground_truth[i + delta] - ground_truth[i]
        
        error = np.linalg.norm(est_rel - gt_rel)
        trans_errors.append(error)
    
    return np.mean(trans_errors), np.std(trans_errors)
```

---

## ğŸ“ í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì´ë¡  ì´í•´

- [ ] Visual Odometryì˜ ì›ë¦¬ë¥¼ ì´í•´í–ˆë‹¤
- [ ] ìŠ¤í…Œë ˆì˜¤ ì‚¼ê°ì¸¡ëŸ‰ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- [ ] PnP ë¬¸ì œì™€ í•´ë²•ì„ ì•Œê³  ìˆë‹¤
- [ ] RANSACì˜ ì—­í• ì„ ì´í•´í–ˆë‹¤

### êµ¬í˜„ ì™„ë£Œ

- [ ] íŠ¹ì§•ì  ê²€ì¶œ/ë§¤ì¹­
- [ ] ìŠ¤í…Œë ˆì˜¤ 3D ì¬êµ¬ì„±
- [ ] PnP ê¸°ë°˜ ëª¨ì…˜ ì¶”ì •
- [ ] ê¶¤ì  ëˆ„ì  ë° ì¶”ì 
- [ ] ì‹œê°í™” (2D/3D)

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ ì‚¬ìš©, ìˆ˜ì •, ë°°í¬ ê°€ëŠ¥

---

## ğŸ‰ ìŠ¤í…Œë ˆì˜¤ ë¹„ì „ êµìœ¡ ì»¤ë¦¬í˜ëŸ¼ ì™„ë£Œ!

ì¶•í•˜í•©ë‹ˆë‹¤! 11ê°œì˜ ëª¨ë“ˆê³¼ 4ê°œì˜ í”„ë¡œì íŠ¸ë¡œ êµ¬ì„±ëœ ìŠ¤í…Œë ˆì˜¤ ë¹„ì „ êµìœ¡ ì»¤ë¦¬í˜ëŸ¼ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.

### ì»¤ë¦¬í˜ëŸ¼ ìš”ì•½

| êµ¬ë¶„ | í•­ëª© | ìƒíƒœ |
|------|------|------|
| **ì´ë¡ ** | Module 01-07 | âœ… ì™„ë£Œ |
| **í”„ë¡œì íŠ¸** | Project 01-04 | âœ… ì™„ë£Œ |

ì´ ì»¤ë¦¬í˜ëŸ¼ì„ í†µí•´ ìŠ¤í…Œë ˆì˜¤ ë¹„ì „ì˜ ê¸°ì´ˆë¶€í„° ê³ ê¸‰ ì‘ìš©ê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
